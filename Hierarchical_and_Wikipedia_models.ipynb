{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanelkaim/RAG/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c1WnQXNLvg2"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu sentence-transformers transformers wikipedia-api torch datasets cohere openai -U langchain-community pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tQq88Wn10q1I"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "import faiss\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, GPT2Tokenizer\n",
        "import cohere\n",
        "import time as tm\n",
        "import openai\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone, ServerlessSpec, PineconeApiException\n",
        "import re\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqS_gy7m1HCm"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "cohere_api_key = \"LjyWoNgE5Cc1E5qytRY90Nwc2VlD1tMdKrkf13nF\"\n",
        "PINECONE_API_KEY = \"pcsk_63k9vT_J3gPSjhkiVxRQi1cn8xJxYg6fBRc7p1DksZS7iNombcuAW3gHNCdRVCuyRWdBqm\"\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# truncate or split any overly long article text into smaller sections before summarization because the model has a maximum input sequence length (1024 tokens)\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", truncation=True)\n",
        "\n",
        "# Load the cross-encoder model for reranking\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "dimension = 384\n",
        "\n",
        "# Global dictionary to store content after indexing\n",
        "wiki_content_map = {}\n",
        "\n",
        "# Hugging Face NER pipeline for keyword extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvqmE28FOLQP",
        "outputId": "1cbd0a5b-c1a8-4ca0-e43b-3e81fccbd7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `ethanelkaim` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `ethanelkaim`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9WqbpOxlqgIk"
      },
      "outputs": [],
      "source": [
        "# Function to extract keywords using Hugging Face's NER pipeline\n",
        "def extract_keywords(query):\n",
        "    ner_results = ner_pipeline(query)\n",
        "    keywords = []\n",
        "    year = None\n",
        "    for entity in ner_results:\n",
        "        entity_word = entity['word']\n",
        "        if entity_word not in keywords and not entity_word.startswith(\"##\"):\n",
        "            keywords.append(entity_word)\n",
        "    year_pattern = r'\\b(?:19|20)\\d{2}\\b' # Extract years\n",
        "    years = re.findall(year_pattern, query)\n",
        "    for year in years:\n",
        "        if year not in keywords:\n",
        "            year = year\n",
        "    return keywords, year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RAG pipeline"
      ],
      "metadata": {
        "id": "MAvBFJu5zPQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "V6EhUP-MNS1E"
      },
      "outputs": [],
      "source": [
        "# Basic Indexing Function\n",
        "def index_dataset_basic(dataset_name, keyword):\n",
        "    \"\"\"Basic indexing of dataset with FAISS.\"\"\"\n",
        "    start_time = tm.time()\n",
        "    global wiki_content_map\n",
        "    wiki_content_map.clear()\n",
        "\n",
        "    if dataset_name == 'wikipedia':\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "        all_paragraphs = []\n",
        "        all_embeddings = []\n",
        "        page_metadata = []\n",
        "\n",
        "        for keyword in keywords:\n",
        "            print(f\"Indexing page: {keyword}\")\n",
        "            page = wiki_wiki.page(keyword)\n",
        "\n",
        "            if page.exists():\n",
        "                paragraphs = page.text.split('\\n')\n",
        "                for idx, paragraph in enumerate(paragraphs):\n",
        "                    if len(paragraph.strip()) > 0:\n",
        "                        # Encode the paragraph and store the embedding\n",
        "                        embedding = model.encode(paragraph, convert_to_tensor=False)\n",
        "                        all_embeddings.append(embedding)  # Append the embedding to our list\n",
        "\n",
        "                        # Store the paragraph and metadata\n",
        "                        all_paragraphs.append(paragraph)\n",
        "                        page_metadata.append({\"keyword\": keyword, \"paragraph_idx\": idx})\n",
        "\n",
        "\n",
        "                print(f\"Indexed page: {keyword}\")\n",
        "            else:\n",
        "                print(f\"Wikipedia page for '{keyword}' does not exist.\")\n",
        "\n",
        "        # Finalize embeddings as a single numpy array for consistency\n",
        "        all_embeddings = np.array(all_embeddings).astype(\"float32\")\n",
        "\n",
        "        # Add metadata for the content in wiki_content_map\n",
        "        wiki_content_map = {\n",
        "            i: {\"paragraph\": all_paragraphs[i], \"metadata\": page_metadata[i]}\n",
        "            for i in range(len(all_paragraphs))\n",
        "        }\n",
        "\n",
        "        runtime = tm.time() - start_time\n",
        "        print(f\"Basic Indexing Time for {dataset_name}: {runtime:.2f} seconds\")\n",
        "        return runtime, wiki_content_map, all_embeddings\n",
        "\n",
        "\n",
        "    elif dataset_name == 'natural_questions':\n",
        "        ds = load_dataset(\"google-research-datasets/natural_questions\", \"default\", split='train[:20]')\n",
        "        embeddings = model.encode(ds['question'])\n",
        "\n",
        "    elif dataset_name == 'cnn_dailymail':\n",
        "      ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train')\n",
        "      embeddings = model.encode(ds['highlights'][:400])\n",
        "\n",
        "    runtime = tm.time() - start_time\n",
        "    print(f\"Basic Indexing Time for {dataset_name}: {runtime:.2f} seconds\")\n",
        "    return runtime, ds, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CIj21lFxG0DE"
      },
      "outputs": [],
      "source": [
        "def create_pinecone_index(index_name: str, dimension: int, metric: str = 'cosine'):\n",
        "    \"\"\"\n",
        "    Create a pinecone index if it does not exist\n",
        "    Args:\n",
        "        index_name: The name of the index\n",
        "        dimension: The dimension of the index\n",
        "        metric: The metric to use for the index\n",
        "    Returns:\n",
        "        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs\n",
        "    \"\"\"\n",
        "    print(\"Creating a Pinecone index...\")\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "    if index_name not in existing_indexes:\n",
        "        pc.create_index(name=index_name, dimension=dimension, metric=metric, spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
        "    print(\"Done!\")\n",
        "    return pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wX1sc89YLQEP"
      },
      "outputs": [],
      "source": [
        "def upsert_vectors(index: Pinecone, embeddings: np.ndarray, dataset, dataset_name: str, text_field: str = 'highlights', batch_size: int = 128):\n",
        "    \"\"\"\n",
        "    Upsert vectors to a Pinecone index with support for multiple dataset formats.\n",
        "    Args:\n",
        "        index: The Pinecone index object.\n",
        "        embeddings: The embeddings to upsert.\n",
        "        dataset: The dataset containing the metadata (dict for 'wikipedia' or Hugging Face Dataset for 'cnn_dailymail').\n",
        "        dataset_name: The name of the dataset being used ('wikipedia' or 'cnn_dailymail').\n",
        "        text_field: The text field in the dataset to use as metadata for 'cnn_dailymail'.\n",
        "        batch_size: The batch size to use for upserting.\n",
        "    Returns:\n",
        "        An updated Pinecone index.\n",
        "    \"\"\"\n",
        "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
        "    shape = embeddings.shape\n",
        "\n",
        "    ids = [str(i) for i in range(shape[0])]\n",
        "\n",
        "    # Prepare metadata based on dataset type\n",
        "    if dataset_name == \"wikipedia\":\n",
        "        meta = [{\"text\": dataset[idx][\"paragraph\"], \"keyword\": dataset[idx][\"metadata\"][\"keyword\"], \"paragraph_idx\": dataset[idx][\"metadata\"][\"paragraph_idx\"]}\n",
        "                for idx in range(len(dataset))]\n",
        "\n",
        "    elif dataset_name == \"cnn_dailymail\":\n",
        "        meta = [{text_field: text} for text in dataset[text_field]]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset name. Use 'wikipedia' or 'cnn_dailymail'.\")\n",
        "\n",
        "    # Create list of (id, vector, metadata) tuples to be upserted\n",
        "    to_upsert = list(zip(ids, embeddings, meta))\n",
        "\n",
        "    # Upsert vectors to Pinecone index in batches\n",
        "    for i in tqdm(range(0, shape[0], batch_size)):\n",
        "        i_end = min(i + batch_size, shape[0])\n",
        "        index.upsert(vectors=to_upsert[i:i_end])\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "68L6yfBrSj4V"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str, model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'), index=None) -> str:\n",
        "    \"\"\"\n",
        "    Augment the prompt with the top 3 results from the knowledge base\n",
        "    Args:\n",
        "        query: The query to augment\n",
        "        index: The vectorstore object\n",
        "    Returns:\n",
        "        str: The augmented prompt\n",
        "    \"\"\"\n",
        "    results = [float(val) for val in list(model.encode(query))]\n",
        "\n",
        "    # get top 3 results from knowledge base\n",
        "    query_results = index.query(\n",
        "        vector=results,\n",
        "        top_k=3,\n",
        "        include_values=True,\n",
        "        include_metadata=True\n",
        "    )['matches']\n",
        "\n",
        "    text_matches = [match['metadata'].get('content', '') for match in query_results]\n",
        "\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\\n\".join(text_matches)\n",
        "\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "    If the answer is not included in the source knowledge - say that you don't know.\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt, source_knowledge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical model"
      ],
      "metadata": {
        "id": "SK_71Z8Ozby1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yV74RBBkGGgE"
      },
      "outputs": [],
      "source": [
        "def encode_dataset_hierarchical(dataset_name, keywords, year, pc, query, chunk_size=1000, chunk_overlap=200, max_tokens=900, max_len=400, doc_len=5):\n",
        "    \"\"\"Hierarchical encoding and indexing with Sentence Transformers and Pinecone for cnn_dailymail or wikipedia datasets.\"\"\"\n",
        "    start_time = tm.time()\n",
        "    documents = []\n",
        "\n",
        "    if dataset_name == 'cnn_dailymail':\n",
        "        ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train')\n",
        "        if year:\n",
        "            documents = [doc for doc in ds if any(keyword.lower() in doc['article'].lower() for keyword in keywords) and str(year) in doc['article']]\n",
        "        else:\n",
        "            documents = [doc for doc in ds if all(keyword.lower() in doc['article'].lower() for keyword in keywords)]\n",
        "\n",
        "    elif dataset_name == 'wikipedia':\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "        for keyword in keywords:\n",
        "            page = wiki_wiki.page(keyword)\n",
        "            if page.exists():\n",
        "                paragraphs = page.text.split('\\n')\n",
        "                for idx, paragraph in enumerate(paragraphs):\n",
        "                    if len(paragraph.strip()) > 0:\n",
        "                        documents.append({\"article\": paragraph, \"id\": f\"{keyword}_{idx}\"})\n",
        "\n",
        "    if len(documents) == 0:\n",
        "        print(\"No documents found for the given keywords and year.\")\n",
        "        return None, None, None, None, None\n",
        "    else:\n",
        "        print(f\"Found {len(documents)} documents.\")\n",
        "\n",
        "    # if len(documents) > doc_len:\n",
        "    #     documents = documents[:doc_len]\n",
        "\n",
        "    # Initialize relevance model\n",
        "    relevance_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "    # Summarize and create embeddings for relevant documents\n",
        "    summaries = []\n",
        "    for doc in tqdm(documents):\n",
        "        article = doc['article']\n",
        "        # print('\\nlen article :',len(article))\n",
        "        # print('article :',article)\n",
        "\n",
        "        # Check relevance\n",
        "        relevance_score = relevance_model.predict([(query, article)])\n",
        "        if len(documents) > 50 and relevance_score < 0.5:\n",
        "            continue\n",
        "        if dataset_name == 'cnn_dailymail':\n",
        "            if len(article.split()) > max_tokens:\n",
        "                chunks = [article[i:i + max_tokens] for i in range(0, len(article), max_tokens - 100)]\n",
        "                summary_parts = [summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "                summary_text = \" \".join(summary_parts)\n",
        "                # print('len summary_text :',len(summary_text))\n",
        "                # print('summary_text ',summary_text)\n",
        "\n",
        "            else:\n",
        "                summary_text = summarizer(article, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
        "                # print('len summary_text :',len(summary_text))\n",
        "                # print('summary_text ',summary_text)\n",
        "\n",
        "        else:\n",
        "            if len(article) > max_len:\n",
        "                summary_text = summarizer(article, max_length=100, min_length=50, do_sample=False)[0]['summary_text']\n",
        "                # print('len summary_text :',len(summary_text))\n",
        "                # print('summary_text ',summary_text)\n",
        "            else:\n",
        "                summary_text = article\n",
        "\n",
        "        summaries.append({\"content\": summary_text, \"source\": doc['id']})\n",
        "\n",
        "    print(f\"Found and summarized {len(summaries)} relevant documents.\")\n",
        "\n",
        "    summary_embeddings = np.array([model.encode(s[\"content\"]) for s in summaries]).astype(\"float32\")\n",
        "\n",
        "    # Define Pinecone index names based on dataset\n",
        "    if dataset_name == \"wikipedia\":\n",
        "        summary_index_name = \"wikipedia-summary\"\n",
        "        chunk_index_name = \"wikipedia-chunk\"\n",
        "    else:\n",
        "        summary_index_name = \"cnn-dailymail-summary\"\n",
        "        chunk_index_name = \"cnn-dailymail-chunk\"\n",
        "\n",
        "    # Helper function to create or connect to Pinecone indexes\n",
        "    def create_or_connect_index(index_name, dimension):\n",
        "        if index_name in pc.list_indexes():\n",
        "            print(f\"Connecting to existing index '{index_name}'.\")\n",
        "            return pc.Index(index_name)\n",
        "        else:\n",
        "            print(f\"Creating new index '{index_name}' with dimension {dimension}.\")\n",
        "            try:\n",
        "                pc.create_index(name=index_name, dimension=dimension, metric='cosine', spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
        "            except PineconeApiException as e:\n",
        "                if 'ALREADY_EXISTS' in str(e):\n",
        "                    print(f\"Index '{index_name}' already exists. Connecting to it.\")\n",
        "                else:\n",
        "                    raise e\n",
        "            return pc.Index(index_name)\n",
        "\n",
        "    # Connect to or create summary and chunk indexes\n",
        "    summary_index = create_or_connect_index(summary_index_name, summary_embeddings.shape[1])\n",
        "    chunk_index = create_or_connect_index(chunk_index_name, summary_embeddings.shape[1])\n",
        "\n",
        "    # Upsert summaries to Pinecone summary index\n",
        "    summary_upserts = [(str(i), summary_embeddings[i], {\"source\": summaries[i][\"source\"]}) for i in range(len(summaries))]\n",
        "    summary_index.upsert(vectors=summary_upserts)\n",
        "\n",
        "    # Split and upsert chunks to Pinecone chunk index\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunks = [chunk for doc in documents for chunk in text_splitter.split_text(doc[\"article\"])]\n",
        "    chunk_embeddings = np.array([model.encode(chunk) for chunk in chunks]).astype(\"float32\")\n",
        "    chunk_upserts = [(str(i), chunk_embeddings[i], {\"content\": chunks[i]}) for i in range(len(chunks))]\n",
        "    chunk_index.upsert(vectors=chunk_upserts)\n",
        "\n",
        "    runtime = tm.time() - start_time\n",
        "    print(f\"Hierarchical Indexing Time for {dataset_name}: {runtime:.2f} seconds\")\n",
        "    return summary_index, chunk_index, summaries, chunks, runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "l218OgyJ-aXn"
      },
      "outputs": [],
      "source": [
        "def retrieve_hierarchical_with_reranking(query: str, summary_index, chunk_index, summaries, chunks, top_k_summaries=3, top_k_chunks=5, rerank_top_n=3) -> Tuple[List[str], float]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant passages using hierarchical indexing with Pinecone,\n",
        "    and rerank the retrieved chunks based on their relevance to the query.\n",
        "    \"\"\"\n",
        "    start_time = tm.time()\n",
        "\n",
        "    # Step 1: Retrieve top summaries\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "    summary_results = summary_index.query(vector=query_embedding, top_k=top_k_summaries, include_values=False, include_metadata=True)\n",
        "    relevant_summaries = [result[\"metadata\"][\"source\"] for result in summary_results[\"matches\"]]\n",
        "\n",
        "    # Step 2: Retrieve relevant chunks from each summary\n",
        "    relevant_chunks = []\n",
        "    for summary in summaries:\n",
        "        if summary[\"source\"] in relevant_summaries:\n",
        "            summary_embedding = model.encode(summary[\"content\"]).tolist()\n",
        "            chunk_results = chunk_index.query(vector=summary_embedding, top_k=top_k_chunks, include_values=False, include_metadata=True)\n",
        "            relevant_chunks.extend([(result[\"metadata\"][\"content\"], query) for result in chunk_results[\"matches\"]])\n",
        "\n",
        "    print(\"relevant_chunks :\\n\", relevant_chunks)\n",
        "\n",
        "    # Step 3: Rerank chunks based on their relevance to the query\n",
        "    if relevant_chunks:\n",
        "        scores = reranker.predict(relevant_chunks)\n",
        "        scored_chunks = sorted(zip(relevant_chunks, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Select the top reranked chunks\n",
        "        reranked_chunks = [chunk[0][0] for chunk in scored_chunks[:rerank_top_n]]\n",
        "    else:\n",
        "        reranked_chunks = []\n",
        "\n",
        "    runtime = tm.time() - start_time\n",
        "    print(f\"Hierarchical Retrieval and Reranking Time: {runtime:.2f} seconds\")\n",
        "    return reranked_chunks, runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9rPRkdr9PrQK"
      },
      "outputs": [],
      "source": [
        "# Function to generate a response using GPT-2\n",
        "def generate_response_gpt2(query):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    generated_text = generator(query, max_length=5000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere's API\n",
        "def generate_response_cohere(query, cohere_api_key):\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.chat(model='command-r-plus', message=query)\n",
        "    return response.text\n",
        "\n",
        "# Function to generate a response using GPT-2 with retrieved context\n",
        "def generate_response_gpt2_with_context(query, retrieved_passages):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    generated_text = generator(context, max_length=10000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere with retrieved context\n",
        "def generate_response_cohere_with_context(query, retrieved_passages, cohere_api_key):\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.generate(prompt=context, model=\"command\").generations[0].text\n",
        "    return response\n",
        "\n",
        "def generate_response_with_augment_prompt(query, retrieved_passages, cohere_api_key, max_tokens=4081):\n",
        "    # Construct initial context\n",
        "    context = query + \"\\n\\nUsing the contexts below, answer the query.\\nContexts:\\n\".join(retrieved_passages)\n",
        "\n",
        "    # Step 1: Check token count\n",
        "    while len(tokenizer.encode(context)) > max_tokens and len(retrieved_passages) > 2:\n",
        "        # Token count exceeds limit. Truncating the last chunk\n",
        "        retrieved_passages.pop()\n",
        "        context = query + \"\\n\\nUsing the contexts below, answer the query.\\nContexts:\\n\".join(retrieved_passages)\n",
        "\n",
        "    # Step 2: Add final prompt instruction\n",
        "    context += \"\\nIf the answer is not included in the source knowledge - say that you don't know.\"\n",
        "\n",
        "    # Initialize Cohere client and generate response\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.generate(prompt=context, model=\"command\").generations[0].text\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbnodSU-UBY2"
      },
      "source": [
        "# Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tr63HLckT_iq"
      },
      "outputs": [],
      "source": [
        "query = \"What can you tell me about NVIDIA's growth and major developments since 2023?\"\n",
        "# query = \"Who won the 2023 Turing Award ?\"\n",
        "# query = \"Who won the 2023 Ballon d'Or for woman ?\"\n",
        "\n",
        "# query = \"How much money did Harry Potter star Daniel Radcliffe have when he was 18?\"\n",
        "# query = \"What was the punishment Michael Vick could face for his role in the dogfighting conspiracy?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "E0ci1KXq2BvM"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"wikipedia\" # \"cnn_dailymail\" or \"wikipedia\" or \"natural_questions\"\n",
        "llm_choice = \"cohere\"  # \"cohere\" or \"gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "B0xlftO82W09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95cd5d9-6fe7-4388-b81a-7940e6b64613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Keywords: ['NVIDIA']\n",
            "Extracted Year: 2023\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Extract keywords from the query\n",
        "keywords, year = extract_keywords(query)\n",
        "print(f\"Extracted Keywords: {keywords}\")\n",
        "if year:\n",
        "  print(f\"Extracted Year: {year}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "B7K4dXRfzzwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gbl4raTJUucZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5b6adf-1122-4e15-88ca-5a1f63fb3119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing page: NVIDIA\n",
            "Indexed page: NVIDIA\n",
            "Basic Indexing Time for wikipedia: 9.03 seconds\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Fetch the data\n",
        "time, ds, embeddings = index_dataset_basic(dataset_name, keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": true,
        "id": "iiZM20-bFqGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d343f979-ac39-4df7-9b3e-c222741dc770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 0 \n",
            "{'paragraph': 'Nvidia Corporation (, en-VID-ee-ə) is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware. It is a software and fabless company which designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) for data science and high-performance computing, as well as system on a chip units (SoCs) for the mobile computing and automotive market. Nvidia is also a dominant supplier of artificial intelligence (AI) hardware and software.', 'metadata': {'keyword': 'NVIDIA', 'paragraph_idx': 0}}\n",
            "Paragraph 1 \n",
            "{'paragraph': \"Nvidia's professional line of GPUs are used for edge-to-cloud computing and in supercomputers and workstations for applications in fields such as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design. Its GeForce line of GPUs are aimed at the consumer market and are used in applications such as video editing, 3D rendering, and PC gaming. With a market share of 80.2% in the second quarter of 2023, Nvidia leads the market for discrete desktop GPUs by a wide margin. The company expanded its presence in the gaming industry with the introduction of the Shield Portable (a handheld game console), Shield Tablet (a gaming tablet), and Shield TV (a digital media player), as well as its cloud gaming service GeForce Now.\", 'metadata': {'keyword': 'NVIDIA', 'paragraph_idx': 1}}\n",
            "Paragraph 2 \n",
            "{'paragraph': 'In addition to GPU design and outsourcing manufacturing, Nvidia provides the CUDA software platform and API that allows the creation of massively parallel programs which utilize GPUs. They are deployed in supercomputing sites around the world. In the late 2000s, Nvidia had moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets as well as vehicle navigation and entertainment systems. Its competitors include AMD, Intel, Qualcomm, and AI accelerator companies such as Cerebras and Graphcore. It also makes AI-powered software for audio and video processing (e.g., Nvidia Maxine).', 'metadata': {'keyword': 'NVIDIA', 'paragraph_idx': 2}}\n",
            "Paragraph 3 \n",
            "{'paragraph': \"Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition. In 2023, Nvidia became the seventh public U.S. company to be valued at over $1 trillion, and the company's valuation has skyrocketed since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom. In June 2024, for one day, Nvidia overtook Microsoft as the world's most valuable publicly traded company, with a market capitalization of over $3.3 trillion.\", 'metadata': {'keyword': 'NVIDIA', 'paragraph_idx': 3}}\n",
            "Paragraph 4 \n",
            "{'paragraph': 'History', 'metadata': {'keyword': 'NVIDIA', 'paragraph_idx': 5}}\n"
          ]
        }
      ],
      "source": [
        "if dataset_name == \"wikipedia\":\n",
        "  for idx, paragraph in wiki_content_map.items():\n",
        "    if idx == 5:\n",
        "      break\n",
        "    print(f\"Paragraph {idx} \\n{paragraph}\")\n",
        "else:\n",
        "  pd_dataset = ds.to_pandas()\n",
        "  print(pd_dataset.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "R_Kt6_z7KBq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955ab3ed-c490-4797-b625-4ada1bfb814e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a Pinecone index...\n",
            "Done!\n",
            "Upserting the embeddings to the Pinecone index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
          ]
        }
      ],
      "source": [
        "if dataset_name == \"wikipedia\":\n",
        "    pc = create_pinecone_index('wikipedia', embeddings.shape[1])\n",
        "    index = pc.Index('wikipedia')\n",
        "    index_upserted = upsert_vectors(index, embeddings, wiki_content_map, dataset_name=\"wikipedia\")\n",
        "else:\n",
        "    pc = create_pinecone_index('cnn-dailymail', embeddings.shape[1])\n",
        "    index = pc.Index('cnn-dailymail')\n",
        "    index_upserted = upsert_vectors(index, embeddings, ds, dataset_name=\"cnn_dailymail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ducWWKTBYHwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c01e8f-9bb2-4a58-9fd5-70298c8c48a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Basic Response (No Retrieval):\n",
            "Unfortunately, as an AI language model, I only have access to the information available on the internet up until the beginning of January 2023. Therefore, I cannot provide details on NVIDIA's growth and developments after that date. However, I can give you an overview of the key strategies and plans that NVIDIA had in place as of early 2023, which may contribute to its future growth and development: \n",
            "\n",
            "- Data Center Business Expansion: NVIDIA has been focusing on expanding its data center business, offering accelerated computing platforms for AI, high-performance computing (HPC), and graphics workloads. The company aims to continue growing this segment by providing innovative solutions to data center customers. \n",
            "\n",
            "- AI for Enterprise: NVIDIA has been promoting the adoption of AI across various industries, including healthcare, finance, retail, and manufacturing. The company offers a range of AI software and hardware solutions to help enterprises accelerate their AI initiatives. \n",
            "\n",
            "- Metaverse and Omniverse Platform: NVIDIA has been heavily investing in the development of its Omniverse platform, which enables the creation of virtual worlds and metaverse applications. The company sees this as a key growth area, with potential applications in gaming, design collaboration, and digital twins. \n",
            "\n",
            "- Autonomous Machines: NVIDIA has been developing its autonomous machine platform, including the NVIDIA Drive platform for autonomous vehicles and the Isaac platform for robotics. The company aims to continue pushing the boundaries of autonomous technology and bring it to a wider range of industries. \n",
            "\n",
            "- Gaming and Professional Visualization: NVIDIA's core business in gaming and professional visualization continues to be a key driver of growth. The company plans to continue innovating in this space with new GPU architectures and software solutions. \n",
            "\n",
            "- Arm Acquisition: NVIDIA's planned acquisition of Arm Limited from SoftBank is expected to be a significant development, pending regulatory approval. This could potentially expand NVIDIA's presence in the CPU market and enable further integration of AI and computing capabilities. \n",
            "\n",
            "- Software and Services: NVIDIA has been increasingly focusing on its software and services business, including AI software frameworks, data center management tools, and cloud-based solutions. The company aims to drive more recurring revenue through this segment. \n",
            "\n",
            "These are some of the key strategies and developments that NVIDIA had in place as of early 2023, which could contribute to its future growth. However, it's important to note that market conditions, competition, and other factors may impact the success and direction of these initiatives. For the most up-to-date information, it is recommended to refer to NVIDIA's official website, news sources, and financial reports.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Basic LLM response (No retrievial context)\n",
        "if llm_choice == \"gpt2\":\n",
        "    basic_response = generate_response_gpt2(query)\n",
        "elif llm_choice == \"cohere\":\n",
        "    if cohere_api_key is None:\n",
        "        raise ValueError(\"Cohere API key is required for Cohere LLM.\")\n",
        "    basic_response = generate_response_cohere(query, cohere_api_key)\n",
        "else:\n",
        "    raise ValueError(\"Invalid LLM choice. Please choose 'gpt2' or 'cohere'.\")\n",
        "\n",
        "print(\"\\nBasic Response (No Retrieval):\")\n",
        "print(basic_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "nMDVbHzP5lw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af34d2b-cca3-4421-f416-d8e684a44d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Here is some information about NVIDIA's growth and major developments between 2023 and now: \n",
            "\n",
            "In late 2023, NVIDIA released its next-generation GPU microarchitecture, dubbed \"Hopper,\" for its NVIDIA Hopper GPU chips. This new architecture introduced several innovations and improvements in performance and energy efficiency for AI and HPC applications. \n",
            "\n",
            "In April of 2024, NVIDIA unveiled \"H100,\" its next-generation GPU architecture, and the successor to the previous generation \"Ampere\" architecture. Some industry experts regarded the H100 as the most significant leap in GPU technology in the last several generations. This new architecture boasts several performance improvements and new features for AI, HPC, and quantum computing applications. \n",
            "\n",
            "In December of that year, NVIDIA announced a major upgrade to its \"Omniverse\" software suite, which is a complex tool suite for simulating and rendering 3D scenes and environments. The upgrade included major updates to allow for simultaneous collaboration on digital twins of real-world locations and objects. \n",
            "\n",
            "In 2025, NVIDIA continued to build on its cybersecurity offerings with the unveiling of its \"Cybersecurity GPU Platform,\" which utilizes AI and deep learning to accelerate and enhance cyber security analytics and workload protection. \n",
            "\n",
            "NVIDIA also expanded its presence in the data center market through significant partnerships with major cloud providers such as AWS, Microsoft, and Google, to offer NVIDIA GPU-accelerated computing for AI, data analytics, and machine learning workloads. \n",
            "\n",
            "As far as overall financial growth, NVIDIA has consistently experienced significant growth in revenue and expansion of its product offerings over the past several years. \n"
          ]
        }
      ],
      "source": [
        "# Step 4: Basic augmented LLM response (with retrieved context)\n",
        "augmented_prompt, source_knowledge = augment_prompt(query, model=model, index=index)\n",
        "\n",
        "if llm_choice == \"gpt2\":\n",
        "    augmented_response = generate_response_gpt2_with_context(query, augmented_prompt)\n",
        "elif llm_choice == \"cohere\":\n",
        "    augmented_response = generate_response_cohere_with_context(query, augmented_prompt, cohere_api_key)\n",
        "\n",
        "print(\"\\nAugmented Response (With Retrieved Context):\")\n",
        "print(augmented_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "id": "HJwkBN61BJir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23f2a82-96ab-408e-ca9a-932080d578fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'indexes': [{'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'wikipedia-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'wikipedia',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}},\n",
            "             {'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'cnn-dailymail-summary-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'cnn-dailymail-summary',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}},\n",
            "             {'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'cnn-dailymail-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'cnn-dailymail',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}},\n",
            "             {'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'cnn-dailymail-chunk-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'cnn-dailymail-chunk',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}}]}\n",
            "Deleted 'cnn-dailymail-summary' index.\n",
            "Deleted 'cnn-dailymail-chunk' index.\n",
            "{'indexes': [{'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'wikipedia-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'wikipedia',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}},\n",
            "             {'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'cnn-dailymail-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'cnn-dailymail',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}}]}\n"
          ]
        }
      ],
      "source": [
        "pc2 = Pinecone(api_key=PINECONE_API_KEY)\n",
        "# List the current indexes\n",
        "print(pc2.list_indexes())\n",
        "\n",
        "# Check if the dataset is 'wikipedia' or 'cnn-dailymail' and delete the relevant indexes if they exist\n",
        "if dataset_name == \"wikipedia\":\n",
        "    if 'cnn-dailymail-summary' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('cnn-dailymail-summary')\n",
        "        print(\"Deleted 'cnn-dailymail-summary' index.\")\n",
        "    if 'cnn-dailymail-chunk' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('cnn-dailymail-chunk')\n",
        "        print(\"Deleted 'cnn-dailymail-chunk' index.\")\n",
        "else:\n",
        "    if 'wikipedia-summary' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('wikipedia-summary')\n",
        "        print(\"Deleted 'wikipedia-summary' index.\")\n",
        "    if 'wikipedia-chunk' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('wikipedia-chunk')\n",
        "        print(\"Deleted 'wikipedia-chunk' index.\")\n",
        "\n",
        "# List the indexes after deletion to confirm changes\n",
        "print(pc2.list_indexes())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Yov4kXhjAHJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9784e608-99d4-4a2b-9756-c239dbeb4ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 168 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 57/168 [00:25<03:54,  2.12s/it]Your max_length is set to 100, but your input_length is only 97. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
            " 35%|███▌      | 59/168 [00:36<05:40,  3.12s/it]Your max_length is set to 100, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
            "100%|██████████| 168/168 [01:21<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found and summarized 16 relevant documents.\n",
            "Creating new index 'wikipedia-summary' with dimension 384.\n",
            "Index 'wikipedia-summary' already exists. Connecting to it.\n",
            "Creating new index 'wikipedia-chunk' with dimension 384.\n",
            "Index 'wikipedia-chunk' already exists. Connecting to it.\n",
            "Hierarchical Indexing Time for wikipedia: 94.70 seconds\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Hierarchical Indexing\n",
        "summary_index, chunk_index, summaries, chunks, hierarchical_runtime = encode_dataset_hierarchical(dataset_name, keywords, year, pc2, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "r4dczNs0_54p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb233b9-dbd8-4187-8534-9fe84d33f087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relevant_chunks :\n",
            " [(\"In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia, then the third most valuable company in the S&P 500, executed a 10-for-1 stock split on June 10, 2024. This move increased the accessibility of shares to investors and followed a significant rise in the company's value, driven by growing demand for its AI-focused semiconductors. The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's 53.4% net margin indicated strong profitability within the technology sector. The company became the world's most valuable, surpassing Microsoft and Apple, on June 18, 2024, after its market capitalization exceeded $3.3 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('On March 1, 2024, Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion. Nvidia needed only 180 days to get to $2 trillion from $1 trillion, while the first two companies, Apple and Microsoft, each took over 500 days. On March 18, Nvidia announced its new AI chip and microarchitecture Blackwell, named after mathematician David Blackwell.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('On November 7, 2024, Nvidia surpassed a $3.6 trillion market value following the U.S. presidential election results.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"For the fiscal year 2020, Nvidia reported earnings of US$2.796 billion, with an annual revenue of US$10.918 billion, a decline of 6.8% over the previous fiscal cycle. Nvidia's shares traded at over $531 per share, and its market capitalization was valued at over US$328.7 billion in January 2021. As of late Q3 2024, Nvidia's market cap is around US$2.98 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition. In 2023, Nvidia became the seventh public U.S. company to be valued at over $1 trillion, and the company's valuation has skyrocketed since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom. In June 2024, for one day, Nvidia overtook Microsoft as the world's most valuable publicly traded company, with a market capitalization of over $3.3 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"For the fiscal year 2020, Nvidia reported earnings of US$2.796 billion, with an annual revenue of US$10.918 billion, a decline of 6.8% over the previous fiscal cycle. Nvidia's shares traded at over $531 per share, and its market capitalization was valued at over US$328.7 billion in January 2021. As of late Q3 2024, Nvidia's market cap is around US$2.98 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia, then the third most valuable company in the S&P 500, executed a 10-for-1 stock split on June 10, 2024. This move increased the accessibility of shares to investors and followed a significant rise in the company's value, driven by growing demand for its AI-focused semiconductors. The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's 53.4% net margin indicated strong profitability within the technology sector. The company became the world's most valuable, surpassing Microsoft and Apple, on June 18, 2024, after its market capitalization exceeded $3.3 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('On March 1, 2024, Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion. Nvidia needed only 180 days to get to $2 trillion from $1 trillion, while the first two companies, Apple and Microsoft, each took over 500 days. On March 18, Nvidia announced its new AI chip and microarchitecture Blackwell, named after mathematician David Blackwell.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('For the Q2 of 2020, Nvidia reported sales of $3.87 billion, which was a 50% rise from the same period in 2019. The surge in sales and people\\'s higher demand for computer technology. According to the financial chief of the company, Colette Kress, the effects of the pandemic will \"likely reflect this evolution in enterprise workforce trends with a greater focus on technologies, such as Nvidia laptops and virtual workstations, that enable remote work and virtual collaboration.\" In May 2023, Nvidia crossed $1 trillion in market valuation during trading hours, and grew to $1.2 trillion by the following November. For its strength, size and market capitalization, Nvidia has been selected to be one of Bloomberg\\'s \"Magnificent Seven\", the seven biggest companies on the stock market in these regards.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('In July 2008, Nvidia took a write-down of approximately $200 million on its first-quarter revenue, after reporting that certain mobile chipsets and GPUs produced by the company had \"abnormal failure rates\" due to manufacturing defects. Nvidia, however, did not reveal the affected products. In September 2008, Nvidia became the subject of a class action lawsuit over the defects, claiming that the faulty GPUs had been incorporated into certain laptop models manufactured by Apple Inc., Dell, and HP. In September 2010, Nvidia reached a settlement, in which it would reimburse owners of the affected laptops for repairs or, in some cases, replacement. On January 10, 2011, Nvidia signed a six-year, $1.5 billion cross-licensing agreement with Intel, ending all litigation between the two companies.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"For the fiscal year 2020, Nvidia reported earnings of US$2.796 billion, with an annual revenue of US$10.918 billion, a decline of 6.8% over the previous fiscal cycle. Nvidia's shares traded at over $531 per share, and its market capitalization was valued at over US$328.7 billion in January 2021. As of late Q3 2024, Nvidia's market cap is around US$2.98 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia, then the third most valuable company in the S&P 500, executed a 10-for-1 stock split on June 10, 2024. This move increased the accessibility of shares to investors and followed a significant rise in the company's value, driven by growing demand for its AI-focused semiconductors. The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's 53.4% net margin indicated strong profitability within the technology sector. The company became the world's most valuable, surpassing Microsoft and Apple, on June 18, 2024, after its market capitalization exceeded $3.3 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('For the Q2 of 2020, Nvidia reported sales of $3.87 billion, which was a 50% rise from the same period in 2019. The surge in sales and people\\'s higher demand for computer technology. According to the financial chief of the company, Colette Kress, the effects of the pandemic will \"likely reflect this evolution in enterprise workforce trends with a greater focus on technologies, such as Nvidia laptops and virtual workstations, that enable remote work and virtual collaboration.\" In May 2023, Nvidia crossed $1 trillion in market valuation during trading hours, and grew to $1.2 trillion by the following November. For its strength, size and market capitalization, Nvidia has been selected to be one of Bloomberg\\'s \"Magnificent Seven\", the seven biggest companies on the stock market in these regards.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), ('On March 1, 2024, Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion. Nvidia needed only 180 days to get to $2 trillion from $1 trillion, while the first two companies, Apple and Microsoft, each took over 500 days. On March 18, Nvidia announced its new AI chip and microarchitecture Blackwell, named after mathematician David Blackwell.', \"What can you tell me about NVIDIA's growth and major developments since 2023?\"), (\"Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition. In 2023, Nvidia became the seventh public U.S. company to be valued at over $1 trillion, and the company's valuation has skyrocketed since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom. In June 2024, for one day, Nvidia overtook Microsoft as the world's most valuable publicly traded company, with a market capitalization of over $3.3 trillion.\", \"What can you tell me about NVIDIA's growth and major developments since 2023?\")]\n",
            "Hierarchical Retrieval and Reranking Time: 2.10 seconds\n",
            "\n",
            "Top Reranked Chunks:\n",
            "\n",
            "Chunk 1:\n",
            "In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia, then the third most valuable company in the S&P 500, executed a 10-for-1 stock split on June 10, 2024. This move increased the accessibility of shares to investors and followed a significant rise in the company's value, driven by growing demand for its AI-focused semiconductors. The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's 53.4% net margin indicated strong profitability within the technology sector. The company became the world's most valuable, surpassing Microsoft and Apple, on June 18, 2024, after its market capitalization exceeded $3.3 trillion.\n",
            "\n",
            "Chunk 2:\n",
            "In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia, then the third most valuable company in the S&P 500, executed a 10-for-1 stock split on June 10, 2024. This move increased the accessibility of shares to investors and followed a significant rise in the company's value, driven by growing demand for its AI-focused semiconductors. The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's 53.4% net margin indicated strong profitability within the technology sector. The company became the world's most valuable, surpassing Microsoft and Apple, on June 18, 2024, after its market capitalization exceeded $3.3 trillion.\n",
            "\n",
            "Chunk 3:\n",
            "In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia, then the third most valuable company in the S&P 500, executed a 10-for-1 stock split on June 10, 2024. This move increased the accessibility of shares to investors and followed a significant rise in the company's value, driven by growing demand for its AI-focused semiconductors. The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's 53.4% net margin indicated strong profitability within the technology sector. The company became the world's most valuable, surpassing Microsoft and Apple, on June 18, 2024, after its market capitalization exceeded $3.3 trillion.\n",
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Sure, here is what I know about Nvidia's growth and major developments since 2023:\n",
            "\n",
            "In June 2024, Nvidia's market capitalization reached $3 trillion for the first time. Nvidia executed a 10-for-1 stock split in June 2024 to make its shares more accessible to investors while its value rose due to increased demand for its AI-focused semiconductors. \n",
            "\n",
            "The company's revenue tripled in the most recent fiscal quarter compared to the previous year, reaching $26 billion, with projections for 2025 nearing $117 billion. Nvidia's profitability was noted to be strong within the technology sector, with a 53.4% net margin. \n",
            "\n",
            "Nvidia surpassed Microsoft and Apple to become the world's most valuable company on June 18, 2024, after its market capitalization exceeded $3.3 trillion. \n",
            "\n",
            "Since these events, there has been no further information publicly available on developments from Nvidia in 2024. Let me know if you would like further information on any of the events I described above! \n"
          ]
        }
      ],
      "source": [
        "# Retrieve relevant chunks using hierarchical retrieval and reranking\n",
        "relevant_chunks, retrieval_time = retrieve_hierarchical_with_reranking(query, summary_index, chunk_index, summaries, chunks)\n",
        "\n",
        "# Print reranked results\n",
        "print(\"\\nTop Reranked Chunks:\")\n",
        "for i, chunk in enumerate(relevant_chunks):\n",
        "    print(f\"\\nChunk {i+1}:\\n{chunk}\")\n",
        "\n",
        "# Now use augmented_prompt in your LLM query\n",
        "augmented_response = generate_response_with_augment_prompt(query, relevant_chunks, cohere_api_key)\n",
        "\n",
        "print(\"\\nAugmented Response (With Retrieved Context):\")\n",
        "print(augmented_response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLDZTV6SnTwNsETRFS0yvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
