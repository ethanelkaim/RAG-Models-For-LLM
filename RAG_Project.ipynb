{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanelkaim/RAG/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c1WnQXNLvg2"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu sentence-transformers transformers wikipedia-api torch datasets cohere openai -U langchain-community pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "tQq88Wn10q1I"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "import faiss\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, GPT2Tokenizer\n",
        "import cohere\n",
        "import time as tm\n",
        "import openai\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone, ServerlessSpec, PineconeApiException\n",
        "import re\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqS_gy7m1HCm",
        "outputId": "16dd402c-29cc-49d0-84fd-b83a5e8d557d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "cohere_api_key = \"LjyWoNgE5Cc1E5qytRY90Nwc2VlD1tMdKrkf13nF\"\n",
        "PINECONE_API_KEY = \"pcsk_63k9vT_J3gPSjhkiVxRQi1cn8xJxYg6fBRc7p1DksZS7iNombcuAW3gHNCdRVCuyRWdBqm\"\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# truncate or split any overly long article text into smaller sections before summarization because the model has a maximum input sequence length (1024 tokens)\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", truncation=True)\n",
        "\n",
        "# Load the cross-encoder model for reranking\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "dimension = 384\n",
        "\n",
        "# Global dictionary to store content after indexing\n",
        "wiki_content_map = {}\n",
        "\n",
        "# Hugging Face NER pipeline for keyword extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvqmE28FOLQP",
        "outputId": "ffeeca10-95ae-4f21-d82d-eb7b5927141f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `ethanelkaim` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `ethanelkaim`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9WqbpOxlqgIk"
      },
      "outputs": [],
      "source": [
        "# Function to extract keywords using Hugging Face's NER pipeline\n",
        "def extract_keywords(query):\n",
        "    ner_results = ner_pipeline(query)\n",
        "    keywords = []\n",
        "    year = None\n",
        "    for entity in ner_results:\n",
        "        entity_word = entity['word']\n",
        "        if entity_word not in keywords and not entity_word.startswith(\"##\"):\n",
        "            keywords.append(entity_word)\n",
        "    year_pattern = r'\\b(?:19|20)\\d{2}\\b' # Extract years\n",
        "    years = re.findall(year_pattern, query)\n",
        "    for year in years:\n",
        "        if year not in keywords:\n",
        "            year = year\n",
        "    return keywords, year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V6EhUP-MNS1E"
      },
      "outputs": [],
      "source": [
        "# Basic Indexing Function\n",
        "def index_dataset_basic(dataset_name, keyword):\n",
        "    \"\"\"Basic indexing of dataset with FAISS.\"\"\"\n",
        "    start_time = tm.time()\n",
        "    global wiki_content_map\n",
        "    wiki_content_map.clear()\n",
        "\n",
        "    if dataset_name == 'wikipedia':\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "        all_paragraphs = []\n",
        "        all_embeddings = []\n",
        "        page_metadata = []\n",
        "\n",
        "        for keyword in keywords:\n",
        "            print(f\"Indexing page: {keyword}\")\n",
        "            page = wiki_wiki.page(keyword)\n",
        "\n",
        "            if page.exists():\n",
        "                paragraphs = page.text.split('\\n')\n",
        "                for idx, paragraph in enumerate(paragraphs):\n",
        "                    if len(paragraph.strip()) > 0:\n",
        "                        # Encode the paragraph and store the embedding\n",
        "                        embedding = model.encode(paragraph, convert_to_tensor=False)\n",
        "                        all_embeddings.append(embedding)  # Append the embedding to our list\n",
        "\n",
        "                        # Store the paragraph and metadata\n",
        "                        all_paragraphs.append(paragraph)\n",
        "                        page_metadata.append({\"keyword\": keyword, \"paragraph_idx\": idx})\n",
        "\n",
        "                        # FAISS expects embeddings as a 2D array, so we convert here\n",
        "                        # index.add(np.array([embedding]))\n",
        "\n",
        "                print(f\"Indexed page: {keyword}\")\n",
        "            else:\n",
        "                print(f\"Wikipedia page for '{keyword}' does not exist.\")\n",
        "\n",
        "        # Finalize embeddings as a single numpy array for consistency\n",
        "        all_embeddings = np.array(all_embeddings).astype(\"float32\")\n",
        "\n",
        "        # Add metadata for the content in wiki_content_map\n",
        "        wiki_content_map = {\n",
        "            i: {\"paragraph\": all_paragraphs[i], \"metadata\": page_metadata[i]}\n",
        "            for i in range(len(all_paragraphs))\n",
        "        }\n",
        "\n",
        "        runtime = tm.time() - start_time\n",
        "        print(f\"Basic Indexing Time for {dataset_name}: {runtime:.2f} seconds\")\n",
        "        return runtime, wiki_content_map, all_embeddings\n",
        "\n",
        "\n",
        "    elif dataset_name == 'natural_questions':\n",
        "        ds = load_dataset(\"google-research-datasets/natural_questions\", \"default\", split='train[:20]')\n",
        "        embeddings = model.encode(ds['question'])\n",
        "\n",
        "    elif dataset_name == 'cnn_dailymail':\n",
        "      ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train')\n",
        "      embeddings = model.encode(ds['highlights'][:400])\n",
        "\n",
        "    runtime = tm.time() - start_time\n",
        "    print(f\"Basic Indexing Time for {dataset_name}: {runtime:.2f} seconds\")\n",
        "    return runtime, ds, embeddings\n",
        "\n",
        "# Function to retrieve the most relevant passages from the indexed content\n",
        "def retrieve_passages(query, top_k=3):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    # Check if retrieved indices have corresponding text passages\n",
        "    retrieved_passages = []\n",
        "    for idx in indices[0]:\n",
        "        if idx in wiki_content_map:\n",
        "            retrieved_passages.append(wiki_content_map[idx])\n",
        "        else:\n",
        "            print(f\"Warning: No passage found for index {idx}\")\n",
        "\n",
        "    if not retrieved_passages:\n",
        "        print(\"No relevant passages found.\")\n",
        "\n",
        "    return retrieved_passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CIj21lFxG0DE"
      },
      "outputs": [],
      "source": [
        "def create_pinecone_index(index_name: str, dimension: int, metric: str = 'cosine'):\n",
        "    \"\"\"\n",
        "    Create a pinecone index if it does not exist\n",
        "    Args:\n",
        "        index_name: The name of the index\n",
        "        dimension: The dimension of the index\n",
        "        metric: The metric to use for the index\n",
        "    Returns:\n",
        "        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs\n",
        "    \"\"\"\n",
        "    print(\"Creating a Pinecone index...\")\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "    if index_name not in existing_indexes:\n",
        "        pc.create_index(name=index_name, dimension=dimension, metric=metric, spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
        "    print(\"Done!\")\n",
        "    return pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wX1sc89YLQEP"
      },
      "outputs": [],
      "source": [
        "def upsert_vectors(index: Pinecone, embeddings: np.ndarray, dataset, dataset_name: str, text_field: str = 'highlights', batch_size: int = 128):\n",
        "    \"\"\"\n",
        "    Upsert vectors to a Pinecone index with support for multiple dataset formats.\n",
        "    Args:\n",
        "        index: The Pinecone index object.\n",
        "        embeddings: The embeddings to upsert.\n",
        "        dataset: The dataset containing the metadata (dict for 'wikipedia' or Hugging Face Dataset for 'cnn_dailymail').\n",
        "        dataset_name: The name of the dataset being used ('wikipedia' or 'cnn_dailymail').\n",
        "        text_field: The text field in the dataset to use as metadata for 'cnn_dailymail'.\n",
        "        batch_size: The batch size to use for upserting.\n",
        "    Returns:\n",
        "        An updated Pinecone index.\n",
        "    \"\"\"\n",
        "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
        "    shape = embeddings.shape\n",
        "\n",
        "    ids = [str(i) for i in range(shape[0])]\n",
        "\n",
        "    # Prepare metadata based on dataset type\n",
        "    if dataset_name == \"wikipedia\":\n",
        "        meta = [{\"text\": dataset[idx][\"paragraph\"], \"keyword\": dataset[idx][\"metadata\"][\"keyword\"], \"paragraph_idx\": dataset[idx][\"metadata\"][\"paragraph_idx\"]}\n",
        "                for idx in range(len(dataset))]\n",
        "\n",
        "    elif dataset_name == \"cnn_dailymail\":\n",
        "        meta = [{text_field: text} for text in dataset[text_field]]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset name. Use 'wikipedia' or 'cnn_dailymail'.\")\n",
        "\n",
        "    # Create list of (id, vector, metadata) tuples to be upserted\n",
        "    to_upsert = list(zip(ids, embeddings, meta))\n",
        "\n",
        "    # Upsert vectors to Pinecone index in batches\n",
        "    for i in tqdm(range(0, shape[0], batch_size)):\n",
        "        i_end = min(i + batch_size, shape[0])\n",
        "        index.upsert(vectors=to_upsert[i:i_end])\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FQrY-muzWJru"
      },
      "outputs": [],
      "source": [
        "# Combined Retrieval Function\n",
        "def retrieve_passages(query, top_k=3, hierarchical=False, summary_index=None, chunk_index=None):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
        "\n",
        "    if hierarchical:\n",
        "        # Use hierarchical retrieval\n",
        "        distances, indices = summary_index.search(np.array([query_embedding]), top_k)\n",
        "        relevant_summaries = [summaries[i][\"content\"] for i in indices[0]]\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = []\n",
        "        for summary_idx in indices[0]:\n",
        "            summary_embedding = np.array([model.encode(summaries[summary_idx][\"content\"])]).astype(\"float32\")\n",
        "            _, chunk_indices = chunk_index.search(summary_embedding, top_k)\n",
        "            relevant_chunks.extend([chunks[i] for i in chunk_indices[0]])\n",
        "\n",
        "        return relevant_chunks\n",
        "    else:\n",
        "        # Basic retrieval\n",
        "        distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "        retrieved_passages = [wiki_content_map[idx] for idx in indices[0] if idx in wiki_content_map]\n",
        "\n",
        "        if not retrieved_passages:\n",
        "            print(\"No relevant passages found.\")\n",
        "\n",
        "        return retrieved_passages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68L6yfBrSj4V"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str, model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'), index=None) -> str:\n",
        "    \"\"\"\n",
        "    Augment the prompt with the top 3 results from the knowledge base\n",
        "    Args:\n",
        "        query: The query to augment\n",
        "        index: The vectorstore object\n",
        "    Returns:\n",
        "        str: The augmented prompt\n",
        "    \"\"\"\n",
        "    results = [float(val) for val in list(model.encode(query))]\n",
        "\n",
        "    # get top 3 results from knowledge base\n",
        "    query_results = index.query(\n",
        "        vector=results,\n",
        "        top_k=3,\n",
        "        include_values=True,\n",
        "        include_metadata=True\n",
        "    )['matches']\n",
        "\n",
        "    # Adjust 'content' or another key if your metadata uses a different key than 'highlights'\n",
        "    text_matches = [match['metadata'].get('content', '') for match in query_results]  # Replace 'content' with the correct key\n",
        "\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\\n\".join(text_matches)\n",
        "\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "    If the answer is not included in the source knowledge - say that you don't know.\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt, source_knowledge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "yV74RBBkGGgE"
      },
      "outputs": [],
      "source": [
        "def encode_dataset_hierarchical(dataset_name, keywords, year, pc, query, chunk_size=1000, chunk_overlap=200, max_tokens=900, max_len=400, doc_len=5):\n",
        "    \"\"\"Hierarchical encoding and indexing with Sentence Transformers and Pinecone for cnn_dailymail or wikipedia datasets.\"\"\"\n",
        "    start_time = tm.time()\n",
        "    documents = []\n",
        "\n",
        "    if dataset_name == 'cnn_dailymail':\n",
        "        ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train')\n",
        "        if year:\n",
        "            documents = [doc for doc in ds if any(keyword.lower() in doc['article'].lower() for keyword in keywords) and str(year) in doc['article']]\n",
        "        else:\n",
        "            documents = [doc for doc in ds if all(keyword.lower() in doc['article'].lower() for keyword in keywords)]\n",
        "\n",
        "    elif dataset_name == 'wikipedia':\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "        for keyword in keywords:\n",
        "            page = wiki_wiki.page(keyword)\n",
        "            if page.exists():\n",
        "                paragraphs = page.text.split('\\n')\n",
        "                for idx, paragraph in enumerate(paragraphs):\n",
        "                    if len(paragraph.strip()) > 0:\n",
        "                        documents.append({\"article\": paragraph, \"id\": f\"{keyword}_{idx}\"})\n",
        "\n",
        "    if len(documents) == 0:\n",
        "        print(\"No documents found for the given keywords and year.\")\n",
        "        return None, None, None, None, None\n",
        "    else:\n",
        "        print(f\"Found {len(documents)} documents.\")\n",
        "\n",
        "    # if len(documents) > doc_len:\n",
        "    #     documents = documents[:doc_len]\n",
        "\n",
        "    # Initialize relevance model\n",
        "    relevance_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "    # Summarize and create embeddings for relevant documents\n",
        "    summaries = []\n",
        "    for doc in tqdm(documents):\n",
        "        article = doc['article']\n",
        "        # print('\\nlen article :',len(article))\n",
        "        # print('article :',article)\n",
        "\n",
        "        # Check relevance\n",
        "        relevance_score = relevance_model.predict([(query, article)])\n",
        "        if relevance_score < 0.5:\n",
        "            continue\n",
        "\n",
        "        if len(article) > max_len:\n",
        "            summary_text = summarizer(article, max_length=100, min_length=50, do_sample=False)[0]['summary_text']\n",
        "            # print('len summary_text :',len(summary_text))\n",
        "            # print('summary_text ',summary_text)\n",
        "        else:\n",
        "            summary_text = article\n",
        "\n",
        "        summaries.append({\"content\": summary_text, \"source\": doc['id']})\n",
        "\n",
        "    print(f\"Found and summarized {len(summaries)} relevant documents.\")\n",
        "\n",
        "    summary_embeddings = np.array([model.encode(s[\"content\"]) for s in summaries]).astype(\"float32\")\n",
        "\n",
        "    # Define Pinecone index names based on dataset\n",
        "    if dataset_name == \"wikipedia\":\n",
        "        summary_index_name = \"wikipedia-summary\"\n",
        "        chunk_index_name = \"wikipedia-chunk\"\n",
        "    else:\n",
        "        summary_index_name = \"cnn-dailymail-summary\"\n",
        "        chunk_index_name = \"cnn-dailymail-chunk\"\n",
        "\n",
        "    # Helper function to create or connect to Pinecone indexes\n",
        "    def create_or_connect_index(index_name, dimension):\n",
        "        if index_name in pc.list_indexes():\n",
        "            print(f\"Connecting to existing index '{index_name}'.\")\n",
        "            return pc.Index(index_name)\n",
        "        else:\n",
        "            print(f\"Creating new index '{index_name}' with dimension {dimension}.\")\n",
        "            try:\n",
        "                pc.create_index(name=index_name, dimension=dimension, metric='cosine', spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
        "            except PineconeApiException as e:\n",
        "                if 'ALREADY_EXISTS' in str(e):\n",
        "                    print(f\"Index '{index_name}' already exists. Connecting to it.\")\n",
        "                else:\n",
        "                    raise e\n",
        "            return pc.Index(index_name)\n",
        "\n",
        "    # Connect to or create summary and chunk indexes\n",
        "    summary_index = create_or_connect_index(summary_index_name, summary_embeddings.shape[1])\n",
        "    chunk_index = create_or_connect_index(chunk_index_name, summary_embeddings.shape[1])\n",
        "\n",
        "    # Upsert summaries to Pinecone summary index\n",
        "    summary_upserts = [(str(i), summary_embeddings[i], {\"source\": summaries[i][\"source\"]}) for i in range(len(summaries))]\n",
        "    summary_index.upsert(vectors=summary_upserts)\n",
        "\n",
        "    # Split and upsert chunks to Pinecone chunk index\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunks = [chunk for doc in documents for chunk in text_splitter.split_text(doc[\"article\"])]\n",
        "    chunk_embeddings = np.array([model.encode(chunk) for chunk in chunks]).astype(\"float32\")\n",
        "    chunk_upserts = [(str(i), chunk_embeddings[i], {\"content\": chunks[i]}) for i in range(len(chunks))]\n",
        "    chunk_index.upsert(vectors=chunk_upserts)\n",
        "\n",
        "    runtime = tm.time() - start_time\n",
        "    print(f\"Hierarchical Indexing Time for {dataset_name}: {runtime:.2f} seconds\")\n",
        "    return summary_index, chunk_index, summaries, chunks, runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "l218OgyJ-aXn"
      },
      "outputs": [],
      "source": [
        "def retrieve_hierarchical_with_reranking(query: str, summary_index, chunk_index, summaries, chunks, top_k_summaries=3, top_k_chunks=5, rerank_top_n=3) -> Tuple[List[str], float]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant passages using hierarchical indexing with Pinecone,\n",
        "    and rerank the retrieved chunks based on their relevance to the query.\n",
        "    \"\"\"\n",
        "    start_time = tm.time()\n",
        "\n",
        "    # Step 1: Retrieve top summaries\n",
        "    query_embedding = model.encode(query).tolist()  # Convert embedding to list\n",
        "    summary_results = summary_index.query(vector=query_embedding, top_k=top_k_summaries, include_values=False, include_metadata=True)\n",
        "    relevant_summaries = [result[\"metadata\"][\"source\"] for result in summary_results[\"matches\"]]\n",
        "\n",
        "    # Step 2: Retrieve relevant chunks from each summary\n",
        "    relevant_chunks = []\n",
        "    for summary in summaries:\n",
        "        if summary[\"source\"] in relevant_summaries:\n",
        "            summary_embedding = model.encode(summary[\"content\"]).tolist()  # Convert embedding to list\n",
        "            chunk_results = chunk_index.query(vector=summary_embedding, top_k=top_k_chunks, include_values=False, include_metadata=True)\n",
        "            relevant_chunks.extend([(result[\"metadata\"][\"content\"], query) for result in chunk_results[\"matches\"]])\n",
        "\n",
        "    print(\"relevant_chunks :\\n\", relevant_chunks)\n",
        "\n",
        "    # Step 3: Rerank chunks based on their relevance to the query\n",
        "    if relevant_chunks:\n",
        "        scores = reranker.predict(relevant_chunks)\n",
        "        scored_chunks = sorted(zip(relevant_chunks, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Select the top reranked chunks\n",
        "        reranked_chunks = [chunk[0][0] for chunk in scored_chunks[:rerank_top_n]]\n",
        "    else:\n",
        "        reranked_chunks = []\n",
        "\n",
        "    runtime = tm.time() - start_time\n",
        "    print(f\"Hierarchical Retrieval and Reranking Time: {runtime:.2f} seconds\")\n",
        "    return reranked_chunks, runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "9rPRkdr9PrQK"
      },
      "outputs": [],
      "source": [
        "# Function to generate a response using GPT-2\n",
        "def generate_response_gpt2(query):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    generated_text = generator(query, max_length=5000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere's API\n",
        "def generate_response_cohere(query, cohere_api_key):\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.chat(model='command-r-plus', message=query)\n",
        "    return response.text\n",
        "\n",
        "# Function to generate a response using GPT-2 with retrieved context\n",
        "def generate_response_gpt2_with_context(query, retrieved_passages):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    generated_text = generator(context, max_length=10000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere with retrieved context\n",
        "def generate_response_cohere_with_context(query, retrieved_passages, cohere_api_key):\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.generate(prompt=context, model=\"command\").generations[0].text\n",
        "    return response\n",
        "\n",
        "def generate_response_with_augment_prompt(query, retrieved_passages, cohere_api_key, max_tokens=4081):\n",
        "    # Construct initial context\n",
        "    context = query + \"\\n\\nUsing the contexts below, answer the query.\\nContexts:\\n\".join(retrieved_passages)\n",
        "\n",
        "    # Step 1: Check token count\n",
        "    while len(tokenizer.encode(context)) > max_tokens and len(retrieved_passages) > 2:\n",
        "        # Token count exceeds limit. Truncating the last chunk\n",
        "        retrieved_passages.pop()\n",
        "        context = query + \"\\n\\nUsing the contexts below, answer the query.\\nContexts:\\n\".join(retrieved_passages)\n",
        "\n",
        "    # Step 2: Add final prompt instruction\n",
        "    context += \"\\nIf the answer is not included in the source knowledge - say that you don't know.\"\n",
        "\n",
        "    # Initialize Cohere client and generate response\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.generate(prompt=context, model=\"command\").generations[0].text\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbnodSU-UBY2"
      },
      "source": [
        "Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "tr63HLckT_iq"
      },
      "outputs": [],
      "source": [
        "# Specific Knowledge:\n",
        "# query = \"How much revenue did Apple generate in Q3 of 2024?\" # Wrong wiki page. the fruit\n",
        "\n",
        "# Requiring Niche or Historical Knowledge:\n",
        "# query = \"What is the significance of the Battle of Kadesh in 1274 BC?\"  # Didn't find the wiki page\n",
        "# query = \"Can you explain the most recent developments in quantum computing?\"  # Didn't find the wiki page\n",
        "\n",
        "# Requiring Information from Niche Datasets:\n",
        "# query = \"What are the requirements to obtain a Brazilian work visa in 2024?\"    # Didn't find the wiki page and basic LLM know the answer\n",
        "# query = \"Who are the top 3 investors in Tesla in 2024?\"\n",
        "\n",
        "# Requiring Up-to-date Pop Culture or Media Knowledge:\n",
        "# query = \"What was the main theme of the latest Marvel movie released in 2024?\"  # Wrong wiki page.\n",
        "# query = \"Which artist won the Oscar for best actor in 2024?\"  # Worked\n",
        "\n",
        "# Complex or Technical Questions Requiring External Sources:\n",
        "# query = \"What are the latest breakthroughs in treating Alzheimer's disease, according to 2024 clinical trials?\"   # Didn't find the wiki page\n",
        "\n",
        "# Questions Requiring Rare or Region-Specific Information:\n",
        "# query = \"What are the local customs of the Himba tribe in Namibia?\"   # Didn't find the wiki page\n",
        "# query = \"How do you brew traditional Mongolian milk tea (Suutei Tsai)?\"  # Basic LLM know the answer\n",
        "\n",
        "query = \"How much money did Harry Potter star Daniel Radcliffe have when he was 18?\"\n",
        "# query = \"What was the punishment Michael Vick could face for his role in the dogfighting conspiracy?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "E0ci1KXq2BvM"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"wikipedia\" # \"cnn_dailymail\" or \"wikipedia\" or \"natural_questions\"\n",
        "llm_choice = \"cohere\"  # \"cohere\" or \"gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "B0xlftO82W09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef74b67c-792f-4880-cb0e-84a0ba8f5f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Keywords: ['Harry Potter', 'Daniel Radcliffe']\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Extract keywords from the query\n",
        "keywords, year = extract_keywords(query)\n",
        "print(f\"Extracted Keywords: {keywords}\")\n",
        "if year:\n",
        "  print(f\"Extracted Year: {year}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "gbl4raTJUucZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd15818-da8c-48f1-8e50-da7f6321bd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing page: Harry Potter\n",
            "Indexed page: Harry Potter\n",
            "Indexing page: Daniel Radcliffe\n",
            "Indexed page: Daniel Radcliffe\n",
            "Basic Indexing Time for wikipedia: 10.89 seconds\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Fetch the data\n",
        "# index = faiss.IndexFlatL2(dimension)\n",
        "time, ds, embeddings = index_dataset_basic(dataset_name, keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "collapsed": true,
        "id": "iiZM20-bFqGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f82a24b-39e8-47f6-cfcc-d8b6f6391cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 0 \n",
            "{'paragraph': \"Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends, Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles (non-magical people).\", 'metadata': {'keyword': 'Harry Potter', 'paragraph_idx': 0}}\n",
            "Paragraph 1 \n",
            "{'paragraph': 'The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States.  A series of many genres, including fantasy, drama, coming-of-age fiction, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references. Major themes in the series include prejudice, corruption, madness, love, and death.', 'metadata': {'keyword': 'Harry Potter', 'paragraph_idx': 1}}\n",
            "Paragraph 2 \n",
            "{'paragraph': \"Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity and commercial success worldwide. They have attracted a wide adult audience as well as younger readers and are widely considered cornerstones of modern literature, though the books have received mixed reviews from critics and literary scholars. As of February 2023, the books have sold more than 600 million copies worldwide, making them the best-selling book series in history, available in dozens of languages. The last four books all set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release.\", 'metadata': {'keyword': 'Harry Potter', 'paragraph_idx': 2}}\n",
            "Paragraph 3 \n",
            "{'paragraph': 'Warner Bros. Pictures adapted the original seven books into an eight-part namesake film series. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion, making it one of the highest-grossing media franchises of all time. Harry Potter and the Cursed Child is a play based on a story co-written by Rowling.', 'metadata': {'keyword': 'Harry Potter', 'paragraph_idx': 3}}\n",
            "Paragraph 4 \n",
            "{'paragraph': 'The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a trilogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments. Themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Destinations & Experiences amusement parks around the world.', 'metadata': {'keyword': 'Harry Potter', 'paragraph_idx': 4}}\n"
          ]
        }
      ],
      "source": [
        "if dataset_name == \"wikipedia\":\n",
        "  for idx, paragraph in wiki_content_map.items():\n",
        "    if idx == 5:\n",
        "      break\n",
        "    print(f\"Paragraph {idx} \\n{paragraph}\")\n",
        "else:\n",
        "  pd_dataset = ds.to_pandas()\n",
        "  pd_dataset.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "R_Kt6_z7KBq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca1824a-3d19-437e-e1b2-a3b601450294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a Pinecone index...\n",
            "Done!\n",
            "Upserting the embeddings to the Pinecone index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n"
          ]
        }
      ],
      "source": [
        "if dataset_name == \"wikipedia\":\n",
        "    pc = create_pinecone_index('wikipedia', embeddings.shape[1])\n",
        "    index = pc.Index('wikipedia')\n",
        "    index_upserted = upsert_vectors(index, embeddings, wiki_content_map, dataset_name=\"wikipedia\")\n",
        "else:\n",
        "    pc = create_pinecone_index('cnn-dailymail', embeddings.shape[1])\n",
        "    index = pc.Index('cnn-dailymail')\n",
        "    index_upserted = upsert_vectors(index, embeddings, ds, dataset_name=\"cnn_dailymail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "3FINDrSSdCHh"
      },
      "outputs": [],
      "source": [
        "queries = [\"How much money did Harry Potter star Daniel Radcliffe have when he was 18?\", query]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "ducWWKTBYHwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd7c29e-64fd-48ee-c76b-7d91893a3e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Basic Response (No Retrieval):\n",
            "Daniel Radcliffe accumulated a net worth of $45 million by the time he was 18 years old. This was primarily due to his earnings from the Harry Potter film series, where he played the titular character. Radcliffe's earnings from the Harry Potter films were significant, and he became one of the highest-paid actors in the world at a young age.\n",
            "\n",
            "It's worth noting that this $45 million figure may include not just his earnings up to that point but also future earnings from projects already contracted, as well as the potential value of deals and endorsements that his stardom could attract.\n",
            "\n",
            "Basic Response (No Retrieval):\n",
            "Daniel Radcliffe accumulated a substantial amount of wealth through his portrayal of Harry Potter in the beloved film series. By the time he turned 18 years old in 2007, his net worth was estimated to be approximately £30 million, which is roughly equivalent to $57 million USD at that time. This estimate includes his earnings from the first five Harry Potter movies, as well as endorsements and other projects.\n",
            "\n",
            "It's worth noting that Daniel Radcliffe's finances were managed by his parents and a team of financial advisors during his younger years, ensuring a stable and secure handling of his earnings.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Basic LLM response (No retrievial context)\n",
        "for query in queries:\n",
        "    if llm_choice == \"gpt2\":\n",
        "        basic_response = generate_response_gpt2(query)\n",
        "    elif llm_choice == \"cohere\":\n",
        "        if cohere_api_key is None:\n",
        "            raise ValueError(\"Cohere API key is required for Cohere LLM.\")\n",
        "        basic_response = generate_response_cohere(query, cohere_api_key)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid LLM choice. Please choose 'gpt2' or 'cohere'.\")\n",
        "\n",
        "    print(\"\\nBasic Response (No Retrieval):\")\n",
        "    print(basic_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "collapsed": true,
        "id": "nMDVbHzP5lw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599895b4-17e5-42f5-968d-678aeac6ffd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Daniel Radcliffe's net worth was approximately $14 million (£10 million) when he was 18 years old. Radcliffe's fortunes came from his role as the lead in the Harry Potter film series between 2001 and 2011, the bulk of his wealth coming from the franchise's lucrative merchandising machine. \n",
            "\n",
            "Since then, Radcliffe has remained a prominent figure in Hollywood, taking on a diverse range of roles that has helped him maintain his financial stability. \n",
            "\n",
            "It's worth noting that these figures are subjective to fluctuation due to various economic factors like inflation and changes in the film industry. \n",
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Daniel Radcliffe's net worth was approximately £18 million when he was 18 years old. Radcliffe reportedly earned £1 million per Potter film during the later stages of the series, and despite his relatively limited experience in the industry, his eight films in the Harry Potter series equate to about £70 million in total. \n",
            "\n",
            "The question didn't specify which currency it intended to use, but given Radcliffe's nationality and the fact that the Harry Potter series was filmed in Britain, it is likely that the currency referenced is the British Pound. \n"
          ]
        }
      ],
      "source": [
        "# Step 4: Basic augmented LLM response (with retrieved context)\n",
        "for query in queries:\n",
        "    augmented_prompt, source_knowledge = augment_prompt(query, model=model, index=index)\n",
        "\n",
        "    if llm_choice == \"gpt2\":\n",
        "        augmented_response = generate_response_gpt2_with_context(query, augmented_prompt)\n",
        "    elif llm_choice == \"cohere\":\n",
        "        augmented_response = generate_response_cohere_with_context(query, augmented_prompt, cohere_api_key)\n",
        "\n",
        "    print(\"\\nAugmented Response (With Retrieved Context):\")\n",
        "    print(augmented_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "collapsed": true,
        "id": "HJwkBN61BJir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6682fed-ede5-40f2-d547-e7652dd163eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'indexes': [{'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'wikipedia-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'wikipedia',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}},\n",
            "             {'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'cnn-dailymail-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'cnn-dailymail',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}}]}\n",
            "{'indexes': [{'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'wikipedia-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'wikipedia',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}},\n",
            "             {'deletion_protection': 'disabled',\n",
            "              'dimension': 384,\n",
            "              'host': 'cnn-dailymail-kv7z9bj.svc.aped-4627-b74a.pinecone.io',\n",
            "              'metric': 'cosine',\n",
            "              'name': 'cnn-dailymail',\n",
            "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
            "              'status': {'ready': True, 'state': 'Ready'}}]}\n"
          ]
        }
      ],
      "source": [
        "pc2 = Pinecone(api_key=PINECONE_API_KEY)\n",
        "# List the current indexes\n",
        "print(pc2.list_indexes())\n",
        "\n",
        "# Check if the dataset is 'wikipedia' or another and delete the relevant indexes if they exist\n",
        "if dataset_name == \"wikipedia\":\n",
        "    if 'cnn-dailymail-summary' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('cnn-dailymail-summary')\n",
        "        print(\"Deleted 'cnn-dailymail-summary' index.\")\n",
        "    if 'cnn-dailymail-chunk' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('cnn-dailymail-chunk')\n",
        "        print(\"Deleted 'cnn-dailymail-chunk' index.\")\n",
        "else:\n",
        "    if 'wikipedia-summary' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('wikipedia-summary')\n",
        "        print(\"Deleted 'wikipedia-summary' index.\")\n",
        "    if 'wikipedia-chunk' in [index.name for index in pc2.list_indexes()]:\n",
        "        pc2.delete_index('wikipedia-chunk')\n",
        "        print(\"Deleted 'wikipedia-chunk' index.\")\n",
        "\n",
        "# List the indexes after deletion to confirm changes\n",
        "print(pc2.list_indexes())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "Yov4kXhjAHJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb4c548-a9c3-40fe-cee6-7159bb5909de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 197 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 197/197 [00:55<00:00,  3.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found and summarized 5 relevant documents.\n",
            "Creating new index 'wikipedia-summary' with dimension 384.\n",
            "Index 'wikipedia-summary' already exists. Connecting to it.\n",
            "Creating new index 'wikipedia-chunk' with dimension 384.\n",
            "Index 'wikipedia-chunk' already exists. Connecting to it.\n",
            "Hierarchical Indexing Time for wikipedia: 69.29 seconds\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Hierarchical Indexing and Retrieval\n",
        "summary_index, chunk_index, summaries, chunks, hierarchical_runtime = encode_dataset_hierarchical(dataset_name, keywords, year, pc2, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "r4dczNs0_54p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70c0cea-277f-4e51-9ae9-ce23383b8ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relevant_chunks :\n",
            " [(\"Daniel Jacob Radcliffe (born 23 July 1989) is an English actor. He rose to fame at age 12 when he began portraying Harry Potter in the Harry Potter film series. Radcliffe plays Potter in all eight films in the series, beginning with Harry Potter and the Philosopher's Stone (2001) and concluding with Harry Potter and the Deathly Hallows – Part 2 (2011).\", 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Radcliffe acknowledged that some people would never be able to separate him from the Harry Potter character; however, he has said he is \"proud to be associated with this film series forever.\" Despite positive feelings about the films, he has no interest in doing more Harry Potter films. After Rowling hinted about writing an eighth book, Radcliffe was asked if he would do another Harry Potter film, to which he replied, \"[It is] very doubtful. I think 10 years is a long time to spend with one character\". Despite devoting so much time to the series, Radcliffe has asserted that he did not miss out on a childhood like other child actors, remarking, \"I\\'ve been given a much better perspective on life by doing Potter.\"', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Daniel Radcliffe at IMDb', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('The release of Harry Potter and the Philosopher\\'s Stone took place in 2001. Radcliffe received a seven-figure salary for the lead role, but asserted that the fee was \"not that important\" to him; his parents chose to invest the money for him. The film was highly popular and was met with positive reviews, and critics took notice of Radcliffe: \"Radcliffe is the embodiment of every reader\\'s imagination. It is wonderful to see a young hero who is so scholarly looking and filled with curiosity and who connects with very real emotions, from solemn intelligence and the delight of discovery to deep family longing,\" wrote Bob Graham of the San Francisco Chronicle.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('In 2002, Radcliffe made his stage debut as a celebrity guest in a West End production of The Play What I Wrote, directed by Kenneth Branagh—who also appeared with him in the second Harry Potter film. He appeared in the film December Boys, an Australian family drama about four orphans that was shot in 2005 and released to theaters in mid-September 2007. On 13 April 2006, a portrait of Radcliffe by Stuart Pearson Wright was unveiled as part of a new exhibition opening at the National Theatre, before being moved to the National Portrait Gallery.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('The release of Harry Potter and the Philosopher\\'s Stone took place in 2001. Radcliffe received a seven-figure salary for the lead role, but asserted that the fee was \"not that important\" to him; his parents chose to invest the money for him. The film was highly popular and was met with positive reviews, and critics took notice of Radcliffe: \"Radcliffe is the embodiment of every reader\\'s imagination. It is wonderful to see a young hero who is so scholarly looking and filled with curiosity and who connects with very real emotions, from solemn intelligence and the delight of discovery to deep family longing,\" wrote Bob Graham of the San Francisco Chronicle.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Harry Potter and the Philosopher\\'s Stone was published by Bloomsbury, the publisher of all Harry Potter books in the United Kingdom, on 26 June 1997. It was released in the United States on 1 September 1998 by Scholastic—the American publisher of the books—as Harry Potter and the Sorcerer\\'s Stone, after the American rights sold for US$105,000—a record amount for a children\\'s book by an unknown author. Scholastic feared that American readers would not associate the word \"philosopher\" with magic, and Rowling suggested the title Harry Potter and the Sorcerer\\'s Stone for the American market. Rowling has later said that she regrets the change.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Radcliffe was reported to have earned £1 million for the first Harry Potter film, around £15 million for the sixth, and around £39 million for the final two movies combined. In all, he is estimated to have made a total of £75.4 million from the entire franchise. He appeared on the Sunday Times Rich List in 2006, which estimated his personal fortune to be £14 million, making him one of the richest young people in the UK. In March 2009, he was ranked at number one on the Forbes \"Most Valuable Young Stars\" list, and by April The Daily Telegraph measured his net worth at £30 million, making him the 12th richest young person in the UK. Radcliffe was considered to be the richest teenager in England later that year. In February 2010, he was named the sixth highest-paid Hollywood male star and placed at number five on Forbes\\' December list of Hollywood\\'s highest-grossing actors with a film revenue of US$780 million, mainly due to Harry Potter and the Deathly Hallows being released that year.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), (\"The Harry Potter series has been recognised by a host of awards since the initial publication of Philosopher's Stone, including a platinum award from the Whitaker Gold and Platinum Book Awards (2001), three Nestlé Smarties Book Prizes (1997–1999), two Scottish Arts Council Book Awards (1999 and 2001), the inaugural Whitbread children's book of the year award (1999), and the WHSmith book of the year (2006), among others. In 2000, Harry Potter and the Prisoner of Azkaban was nominated for a Hugo Award for Best Novel, and in 2001, Harry Potter and the Goblet of Fire won said award. Honours include a commendation for the Carnegie Medal (1997), a short listing for the Guardian Children's Award (1998), and numerous listings on the notable books, editors' choices, and best books lists of the American Library Association, The New York Times, Chicago Public Library, and Publishers Weekly.\", 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Early in its history, Harry Potter received positive reviews. On publication, the first book, Harry Potter and the Philosopher\\'s Stone, attracted attention from the Scottish newspapers, such as The Scotsman, which said it had \"all the makings of a classic\", and The Glasgow Herald, which called it \"Magic stuff\". Soon the English newspapers joined in, with The Sunday Times comparing it to Roald Dahl\\'s work (\"comparisons to Dahl are, this time, justified\"), while The Guardian called it \"a richly textured novel given lift-off by an inventive wit\".', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Radcliffe was reported to have earned £1 million for the first Harry Potter film, around £15 million for the sixth, and around £39 million for the final two movies combined. In all, he is estimated to have made a total of £75.4 million from the entire franchise. He appeared on the Sunday Times Rich List in 2006, which estimated his personal fortune to be £14 million, making him one of the richest young people in the UK. In March 2009, he was ranked at number one on the Forbes \"Most Valuable Young Stars\" list, and by April The Daily Telegraph measured his net worth at £30 million, making him the 12th richest young person in the UK. Radcliffe was considered to be the richest teenager in England later that year. In February 2010, he was named the sixth highest-paid Hollywood male star and placed at number five on Forbes\\' December list of Hollywood\\'s highest-grossing actors with a film revenue of US$780 million, mainly due to Harry Potter and the Deathly Hallows being released that year.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), (\"at number five on Forbes' December list of Hollywood's highest-grossing actors with a film revenue of US$780 million, mainly due to Harry Potter and the Deathly Hallows being released that year. As of 2021, Radcliffe's net worth is estimated at £95 million.\", 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('The release of Harry Potter and the Philosopher\\'s Stone took place in 2001. Radcliffe received a seven-figure salary for the lead role, but asserted that the fee was \"not that important\" to him; his parents chose to invest the money for him. The film was highly popular and was met with positive reviews, and critics took notice of Radcliffe: \"Radcliffe is the embodiment of every reader\\'s imagination. It is wonderful to see a young hero who is so scholarly looking and filled with curiosity and who connects with very real emotions, from solemn intelligence and the delight of discovery to deep family longing,\" wrote Bob Graham of the San Francisco Chronicle.', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), ('Radcliffe acknowledged that some people would never be able to separate him from the Harry Potter character; however, he has said he is \"proud to be associated with this film series forever.\" Despite positive feelings about the films, he has no interest in doing more Harry Potter films. After Rowling hinted about writing an eighth book, Radcliffe was asked if he would do another Harry Potter film, to which he replied, \"[It is] very doubtful. I think 10 years is a long time to spend with one character\". Despite devoting so much time to the series, Radcliffe has asserted that he did not miss out on a childhood like other child actors, remarking, \"I\\'ve been given a much better perspective on life by doing Potter.\"', 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?'), (\"Daniel Jacob Radcliffe (born 23 July 1989) is an English actor. He rose to fame at age 12 when he began portraying Harry Potter in the Harry Potter film series. Radcliffe plays Potter in all eight films in the series, beginning with Harry Potter and the Philosopher's Stone (2001) and concluding with Harry Potter and the Deathly Hallows – Part 2 (2011).\", 'How much money did Harry Potter star Daniel Radcliffe have when he was 18?')]\n",
            "Hierarchical Retrieval and Reranking Time: 1.73 seconds\n",
            "\n",
            "Top Reranked Chunks:\n",
            "\n",
            "Chunk 1:\n",
            "Daniel Jacob Radcliffe (born 23 July 1989) is an English actor. He rose to fame at age 12 when he began portraying Harry Potter in the Harry Potter film series. Radcliffe plays Potter in all eight films in the series, beginning with Harry Potter and the Philosopher's Stone (2001) and concluding with Harry Potter and the Deathly Hallows – Part 2 (2011).\n",
            "\n",
            "Chunk 2:\n",
            "Daniel Jacob Radcliffe (born 23 July 1989) is an English actor. He rose to fame at age 12 when he began portraying Harry Potter in the Harry Potter film series. Radcliffe plays Potter in all eight films in the series, beginning with Harry Potter and the Philosopher's Stone (2001) and concluding with Harry Potter and the Deathly Hallows – Part 2 (2011).\n",
            "\n",
            "Chunk 3:\n",
            "Radcliffe was reported to have earned £1 million for the first Harry Potter film, around £15 million for the sixth, and around £39 million for the final two movies combined. In all, he is estimated to have made a total of £75.4 million from the entire franchise. He appeared on the Sunday Times Rich List in 2006, which estimated his personal fortune to be £14 million, making him one of the richest young people in the UK. In March 2009, he was ranked at number one on the Forbes \"Most Valuable Young Stars\" list, and by April The Daily Telegraph measured his net worth at £30 million, making him the 12th richest young person in the UK. Radcliffe was considered to be the richest teenager in England later that year. In February 2010, he was named the sixth highest-paid Hollywood male star and placed at number five on Forbes' December list of Hollywood's highest-grossing actors with a film revenue of US$780 million, mainly due to Harry Potter and the Deathly Hallows being released that year.\n",
            "\n",
            "Source Knowledge:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Daniel Radcliffe is estimated to have made a total of £75.4 million from the entire Harry Potter franchise. At 18, Radcliffe was worth approximately 30 million GBP, established from various roles preceding his 18th birthday. \n"
          ]
        }
      ],
      "source": [
        "# Retrieve relevant chunks using hierarchical retrieval and reranking\n",
        "relevant_chunks, retrieval_time = retrieve_hierarchical_with_reranking(query, summary_index, chunk_index, summaries, chunks)\n",
        "\n",
        "# Print reranked results\n",
        "print(\"\\nTop Reranked Chunks:\")\n",
        "for i, chunk in enumerate(relevant_chunks):\n",
        "    print(f\"\\nChunk {i+1}:\\n{chunk}\")\n",
        "\n",
        "# Now use augmented_prompt in your LLM query\n",
        "augmented_response = generate_response_with_augment_prompt(query, relevant_chunks, cohere_api_key)\n",
        "\n",
        "print(\"\\nSource Knowledge:\")\n",
        "print(source_knowledge)\n",
        "\n",
        "print(\"\\nAugmented Response (With Retrieved Context):\")\n",
        "print(augmented_response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtqzXECwBDgC+X5ERvbcE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}