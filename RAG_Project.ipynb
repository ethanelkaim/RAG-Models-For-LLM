{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHegqkxmoAsQi2zgFgEHVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanelkaim/RAG/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers transformers wikipedia-api torch datasets cohere"
      ],
      "metadata": {
        "id": "6c1WnQXNLvg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import faiss\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "import cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQq88Wn10q1I",
        "outputId": "72e1c771-06af-4d30-a696-2e8e41bceacf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the sentence transformer model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# FAISS Index (for vector-based retrieval)\n",
        "dimension = 384\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Hugging Face NER pipeline for keyword extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
        "\n",
        "# Function to extract keywords using Hugging Face's NER pipeline\n",
        "def extract_keywords(query):\n",
        "    ner_results = ner_pipeline(query)\n",
        "    keywords = []\n",
        "    for entity in ner_results:\n",
        "        entity_word = entity['word']\n",
        "        if entity_word not in keywords and not entity_word.startswith(\"##\"):\n",
        "            keywords.append(entity_word)\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "UqS_gy7m1HCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global dictionary to store content after indexing\n",
        "wiki_content_map = {}\n",
        "\n",
        "# Function to fetch content and embeddings from the dataset\n",
        "def index_dataset(dataset_name, keyword):\n",
        "    global wiki_content_map\n",
        "    wiki_content_map.clear()  # Clear the map for each new query\n",
        "\n",
        "    if dataset_name == 'wikipedia':\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "        page = wiki_wiki.page(keyword)\n",
        "        if page.exists():\n",
        "            paragraphs = page.text.split('\\n')\n",
        "            for idx, paragraph in enumerate(paragraphs):\n",
        "                if len(paragraph.strip()) > 0:\n",
        "                    # print(f\"Paragraph \\n{paragraph}\")\n",
        "                    embedding = model.encode(paragraph, convert_to_tensor=False)\n",
        "                    embedding = np.array([embedding])  # FAISS requires 2D arrays\n",
        "                    index.add(embedding)\n",
        "                    wiki_content_map[idx] = paragraph  # Store the paragraph in the content map\n",
        "            print(f\"Indexed page: {keyword}\")\n",
        "        else:\n",
        "            print(f\"Wikipedia page for '{keyword}' does not exist.\")\n",
        "\n",
        "    elif dataset_name == 'natural_questions':\n",
        "        ds = load_dataset(\"google-research-datasets/natural_questions\", \"default\")\n",
        "        for i, example in enumerate(ds['train']):\n",
        "            if keyword.lower() in example['question'].lower():\n",
        "                passage = example['document_text']\n",
        "                embedding = model.encode(passage, convert_to_tensor=False)\n",
        "                embedding = np.array([embedding])  # FAISS requires 2D arrays\n",
        "                index.add(embedding)\n",
        "                wiki_content_map[i] = passage  # Store the passage in the content map\n",
        "        print(f\"Indexed examples from Natural Questions for keyword: {keyword}\")\n",
        "\n",
        "    elif dataset_name == 'cnn_dailymail':\n",
        "        ds = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "        for i, example in enumerate(ds['train']):\n",
        "            if keyword.lower() in example['article'].lower():\n",
        "                passage = example['article']\n",
        "                embedding = model.encode(passage, convert_to_tensor=False)\n",
        "                embedding = np.array([embedding])  # FAISS requires 2D arrays\n",
        "                index.add(embedding)\n",
        "                wiki_content_map[i] = passage  # Store the passage in the content map\n",
        "        print(f\"Indexed examples from CNN/DailyMail for keyword: {keyword}\")\n",
        "\n",
        "# Function to retrieve the most relevant passages from the indexed content\n",
        "def retrieve_passages(query, top_k=3):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    # Check if retrieved indices have corresponding text passages\n",
        "    retrieved_passages = []\n",
        "    for idx in indices[0]:\n",
        "        if idx in wiki_content_map:\n",
        "            retrieved_passages.append(wiki_content_map[idx])\n",
        "        else:\n",
        "            print(f\"Warning: No passage found for index {idx}\")\n",
        "\n",
        "    if not retrieved_passages:\n",
        "        print(\"No relevant passages found.\")\n",
        "\n",
        "    return retrieved_passages"
      ],
      "metadata": {
        "id": "V6EhUP-MNS1E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response using GPT-2\n",
        "def generate_response_gpt2(query):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    generated_text = generator(query, max_length=5000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere's API\n",
        "def generate_response_cohere(query, cohere_api_key):\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.chat(model='command-r-plus', message=query)\n",
        "    return response.text\n",
        "\n",
        "# Function to generate a response using GPT-2 with retrieved context\n",
        "def generate_response_gpt2_with_context(query, retrieved_passages):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    generated_text = generator(context, max_length=10000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere with retrieved context\n",
        "def generate_response_cohere_with_context(query, retrieved_passages, cohere_api_key):\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.generate(prompt=context, model=\"command\").generations[0].text\n",
        "    return response"
      ],
      "metadata": {
        "id": "9rPRkdr9PrQK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvqmE28FOLQP",
        "outputId": "bcc5f49d-6c43-45f4-d6ef-65f8492d7e18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queries"
      ],
      "metadata": {
        "id": "rbnodSU-UBY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specific Knowledge:\n",
        "# query = \"How much revenue did Apple generate in Q3 of 2024?\" # Wrong wiki page. the fruit\n",
        "\n",
        "# Requiring Niche or Historical Knowledge:\n",
        "# query = \"What is the significance of the Battle of Kadesh in 1274 BC?\"  # Didn't find the wiki page\n",
        "# query = \"Can you explain the most recent developments in quantum computing?\"  # Didn't find the wiki page\n",
        "\n",
        "# Requiring Information from Niche Datasets:\n",
        "# query = \"What are the requirements to obtain a Brazilian work visa in 2024?\"    # Didn't find the wiki page and basic LLM know the answer\n",
        "query = \"Who are the top 3 investors in Tesla in 2024?\"\n",
        "\n",
        "# Requiring Up-to-date Pop Culture or Media Knowledge:\n",
        "# query = \"What was the main theme of the latest Marvel movie released in 2024?\"  # Wrong wiki page.\n",
        "# query = \"Which artist won the Grammy for Best Album in 2024?\"  # Worked\n",
        "\n",
        "# Complex or Technical Questions Requiring External Sources:\n",
        "# query = \"What are the latest breakthroughs in treating Alzheimer's disease, according to 2023 clinical trials?\"   # Didn't find the wiki page\n",
        "\n",
        "# Questions Requiring Rare or Region-Specific Information:\n",
        "# query = \"What are the local customs of the Himba tribe in Namibia?\"   # Didn't find the wiki page\n",
        "# query = \"How do you brew traditional Mongolian milk tea (Suutei Tsai)?\"  # Basic LLM know the answer"
      ],
      "metadata": {
        "id": "tr63HLckT_iq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"cnn_dailymail\" # or \"wikipedia\"  # or \"natural_questions\"\n",
        "llm_choice = \"cohere\"  # or \"gpt2\"\n",
        "cohere_api_key = \"LjyWoNgE5Cc1E5qytRY90Nwc2VlD1tMdKrkf13nF\""
      ],
      "metadata": {
        "id": "E0ci1KXq2BvM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract keywords from the query\n",
        "keywords = extract_keywords(query)\n",
        "print(f\"Extracted Keywords: {keywords}\")\n",
        "\n",
        "# Step 2: Fetch the data (Wikipedia or Natural Questions)\n",
        "for keyword in keywords:\n",
        "    content_map = index_dataset(dataset_name, keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0xlftO82W09",
        "outputId": "3440156e-ea4b-4a56-a022-8f6488d7fe49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Keywords: ['Apple']\n",
            "Indexed examples from CNN/DailyMail for keyword: Apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Retrieve the most relevant passages\n",
        "retrieved_passages = retrieve_passages(query, top_k=3)\n",
        "print(\"Retrieved Passages:\")\n",
        "for passage in retrieved_passages:\n",
        "    print(passage)\n",
        "\n",
        "# Step 4: Basic LLM response (without retrieved context)\n",
        "if llm_choice == \"gpt2\":\n",
        "    basic_response = generate_response_gpt2(query)\n",
        "elif llm_choice == \"cohere\":\n",
        "    if cohere_api_key is None:\n",
        "        raise ValueError(\"Cohere API key is required for Cohere LLM.\")\n",
        "    basic_response = generate_response_cohere(query, cohere_api_key)\n",
        "else:\n",
        "    raise ValueError(\"Invalid LLM choice. Please choose 'gpt2' or 'cohere'.\")\n",
        "\n",
        "print(\"\\nBasic Response (No Retrieval):\")\n",
        "print(basic_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ducWWKTBYHwE",
        "outputId": "e15dca74-5e7c-440e-b7c1-af0b80189f32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No passage found for index 7895\n",
            "Warning: No passage found for index 3885\n",
            "Warning: No passage found for index 6268\n",
            "No relevant passages found.\n",
            "Retrieved Passages:\n",
            "\n",
            "Basic Response (No Retrieval):\n",
            "Apple Inc.'s financial results for the third quarter of 2023 are not yet publicly available as I have information on events only up to January 2023. The third quarter results will be typically announced a few weeks after the end of the quarter, which is September 30th. You can expect the announcement in early October 2023. \n",
            "\n",
            "I can provide you with Apple's financial results for the most recent quarter, Q3 of their 2022 fiscal year (which ended on July 2, 2022) if you are interested.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Augmented LLM response (with retrieved context)\n",
        "if llm_choice == \"gpt2\":\n",
        "    augmented_response = generate_response_gpt2_with_context(query, retrieved_passages)\n",
        "elif llm_choice == \"cohere\":\n",
        "    augmented_response = generate_response_cohere_with_context(query, retrieved_passages, cohere_api_key)\n",
        "\n",
        "print(\"\\nAugmented Response (With Retrieved Context):\")\n",
        "print(augmented_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMDVbHzP5lw6",
        "outputId": "4ea57604-6e08-4a20-b334-b58b399e1cc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Apple reported revenue of $29.9 billion in its third fiscal quarter of 2023, which represents a 2% increase compared to the same period in 2022. This result was largely driven by robust performance in the company's product categories, particularly its iPhone and Mac product lines. The company's continued success in these segments highlights its ability to innovate and meet the evolving needs of its global customer base, even in the face of a challenging macroeconomic backdrop. \n",
            "\n",
            "It's worth noting that Apple also posted impressive earnings per share (EPS) of $1.19 during this quarter, which exceeded the expected EPS of $1.16. This result demonstrates that Apple continues to effectively manage its operations, optimize its cost structure, and invest in high-growth areas, thereby boosting shareholder value. \n",
            "\n",
            "In summary, Apple's Q3 2023 earnings reflect its sustained performance and ongoing success in the technology industry. The company's ability to drive revenue growth and maintain profitability, even amid competitive and economic challenges, underscores its strong market position and the resilience of its business model. \n"
          ]
        }
      ]
    }
  ]
}