{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc4VR/xRCKy2cx4oCzcwbd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanelkaim/RAG/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers transformers wikipedia-api torch datasets cohere"
      ],
      "metadata": {
        "id": "6c1WnQXNLvg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import faiss\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "import cohere"
      ],
      "metadata": {
        "id": "tQq88Wn10q1I"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the sentence transformer model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# FAISS Index (for vector-based retrieval)\n",
        "dimension = 384\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Hugging Face NER pipeline for keyword extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqS_gy7m1HCm",
        "outputId": "94fd665c-8deb-4e26-a982-4921c6644ae5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract keywords using Hugging Face's NER pipeline\n",
        "def extract_keywords(query):\n",
        "    ner_results = ner_pipeline(query)\n",
        "    keywords = []\n",
        "    for entity in ner_results:\n",
        "        entity_word = entity['word']\n",
        "        if entity_word not in keywords and not entity_word.startswith(\"##\"):\n",
        "            keywords.append(entity_word)\n",
        "    return keywords\n",
        "\n",
        "# Global dictionary to store content after indexing\n",
        "wiki_content_map = {}\n",
        "\n",
        "# Function to fetch content and embeddings from the dataset\n",
        "def index_dataset(dataset_name, keyword):\n",
        "    global wiki_content_map\n",
        "    wiki_content_map.clear()  # Clear the map for each new query\n",
        "\n",
        "    if dataset_name == 'wikipedia':\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "        page = wiki_wiki.page(keyword)\n",
        "        if page.exists():\n",
        "            paragraphs = page.text.split('\\n')\n",
        "            for idx, paragraph in enumerate(paragraphs):\n",
        "                if len(paragraph.strip()) > 0:\n",
        "                    embedding = model.encode(paragraph, convert_to_tensor=False)\n",
        "                    embedding = np.array([embedding])  # FAISS requires 2D arrays\n",
        "                    index.add(embedding)\n",
        "                    wiki_content_map[idx] = paragraph  # Store the paragraph in the content map\n",
        "            print(f\"Indexed page: {keyword}\")\n",
        "        else:\n",
        "            print(f\"Wikipedia page for '{keyword}' does not exist.\")\n",
        "    elif dataset_name == 'natural_questions':\n",
        "        ds = load_dataset(\"google-research-datasets/natural_questions\", \"default\")\n",
        "        for i, example in enumerate(ds['train']):\n",
        "            if keyword.lower() in example['question'].lower():\n",
        "                passage = example['document_text']\n",
        "                embedding = model.encode(passage, convert_to_tensor=False)\n",
        "                embedding = np.array([embedding])  # FAISS requires 2D arrays\n",
        "                index.add(embedding)\n",
        "                wiki_content_map[i] = passage  # Store the passage in the content map\n",
        "        print(f\"Indexed examples from Natural Questions for keyword: {keyword}\")\n",
        "\n",
        "# Function to retrieve the most relevant passages from the indexed content\n",
        "def retrieve_passages(query, top_k=3):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    # Check if retrieved indices have corresponding text passages\n",
        "    retrieved_passages = []\n",
        "    for idx in indices[0]:\n",
        "        if idx in wiki_content_map:\n",
        "            retrieved_passages.append(wiki_content_map[idx])\n",
        "        else:\n",
        "            print(f\"Warning: No passage found for index {idx}\")\n",
        "\n",
        "    if not retrieved_passages:\n",
        "        print(\"No relevant passages found.\")\n",
        "\n",
        "    return retrieved_passages"
      ],
      "metadata": {
        "id": "V6EhUP-MNS1E"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response using GPT-2\n",
        "def generate_response_gpt2(query):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    generated_text = generator(query, max_length=5000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere's API\n",
        "def generate_response_cohere(query, cohere_api_key):\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.chat(model='command-r-plus', message=query)\n",
        "    return response.text\n",
        "\n",
        "# Function to generate a response using GPT-2 with retrieved context\n",
        "def generate_response_gpt2_with_context(query, retrieved_passages):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    generated_text = generator(context, max_length=10000, num_return_sequences=1)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate a response using Cohere with retrieved context\n",
        "def generate_response_cohere_with_context(query, retrieved_passages, cohere_api_key):\n",
        "    context = query + \"\\n\\n\" + \"\\n\".join(retrieved_passages)\n",
        "    co = cohere.Client(api_key=cohere_api_key)\n",
        "    response = co.generate(prompt=context, model=\"command\").generations[0].text\n",
        "    return response"
      ],
      "metadata": {
        "id": "9rPRkdr9PrQK"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvqmE28FOLQP",
        "outputId": "afb59a4a-57f7-4147-a694-f71cc5dfb80a"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the Leonardo DiCaprio mother's name?\"\n",
        "# query = \"How much money did Harry Potter star Daniel Radcliffe have when he was 18?\"\n",
        "dataset_name = \"wikipedia\"  # or \"natural_questions\"\n",
        "llm_choice = \"cohere\"  # or \"gpt2\"\n",
        "cohere_api_key = \"LjyWoNgE5Cc1E5qytRY90Nwc2VlD1tMdKrkf13nF\""
      ],
      "metadata": {
        "id": "E0ci1KXq2BvM"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract keywords from the query\n",
        "keywords = extract_keywords(query)\n",
        "print(f\"Extracted Keywords: {keywords}\")\n",
        "\n",
        "# Step 2: Fetch the data (Wikipedia or Natural Questions)\n",
        "for keyword in keywords:\n",
        "    content_map = index_dataset(dataset_name, keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0xlftO82W09",
        "outputId": "5dc27813-776c-4ead-d37a-024afde54000"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Keywords: ['Leonardo DiCaprio']\n",
            "Indexed page: Leonardo DiCaprio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Retrieve the most relevant passages\n",
        "retrieved_passages = retrieve_passages(query, top_k=3)\n",
        "print(\"Retrieved Passages:\")\n",
        "for passage in retrieved_passages:\n",
        "    print(passage)\n",
        "\n",
        "# Step 4: Basic LLM response (without retrieved context)\n",
        "if llm_choice == \"gpt2\":\n",
        "    basic_response = generate_response_gpt2(query)\n",
        "elif llm_choice == \"cohere\":\n",
        "    if cohere_api_key is None:\n",
        "        raise ValueError(\"Cohere API key is required for Cohere LLM.\")\n",
        "    basic_response = generate_response_cohere(query, cohere_api_key)\n",
        "else:\n",
        "    raise ValueError(\"Invalid LLM choice. Please choose 'gpt2' or 'cohere'.\")\n",
        "\n",
        "print(\"\\nBasic Response (No Retrieval):\")\n",
        "print(basic_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ducWWKTBYHwE",
        "outputId": "7444a28d-0bea-4ccb-a1ba-079dd27d30ce"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No passage found for index 381\n",
            "Retrieved Passages:\n",
            "Leonardo Wilhelm DiCaprio was born on November 11, 1974, in Los Angeles, California. He is the only child of Irmelin Indenbirken, a legal secretary, and George DiCaprio, an underground comix artist and distributor. They met while attending college and moved to Los Angeles after graduating. His mother is German and his father is of Italian and German descent. His maternal grandfather, Wilhelm Indenbirken, was German, and his maternal grandmother, Helene Indenbirken, was a Russian immigrant living in Germany. DiCaprio was raised Catholic. Sources have falsely claimed his maternal grandmother was born in Odesa, Ukraine; there is no evidence that DiCaprio has any relatives of Ukrainian birth or heritage.\n",
            "See also\n",
            "\n",
            "Basic Response (No Retrieval):\n",
            "Irmelin DiCaprio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Augmented LLM response (with retrieved context)\n",
        "if llm_choice == \"gpt2\":\n",
        "    augmented_response = generate_response_gpt2_with_context(query, retrieved_passages)\n",
        "elif llm_choice == \"cohere\":\n",
        "    augmented_response = generate_response_cohere_with_context(query, retrieved_passages, cohere_api_key)\n",
        "\n",
        "print(\"\\nAugmented Response (With Retrieved Context):\")\n",
        "print(augmented_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMDVbHzP5lw6",
        "outputId": "073011a5-d6c5-4b97-e641-95d7a8fef3a3"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmented Response (With Retrieved Context):\n",
            " Irmelin Indenbirken is Leonardo DiCaprio's mother's name. \n",
            "Would you like help with anything else?  I can also provide more information if you'd like. \n"
          ]
        }
      ]
    }
  ]
}